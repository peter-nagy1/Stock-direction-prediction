{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python37264bitc69858c3cc064b3a811d50b9ddeffc56",
      "display_name": "Python 3.7.2 64-bit"
    },
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "metadata": {
      "interpreter": {
        "hash": "5163c4f78c61f41f5bea3570cd2b8132cf5ff719c06498f8f1e41128c011b2cc"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZgP-yFNzfK8"
      },
      "source": [
        "# Stock price direction prediction\n",
        "\n",
        "##Absract\n",
        "For this project, we decided to work on the prediction of future stock price using custom built Keras densely-connected Neural Network models and long short-term memory (LSTM) models. We present our data's source and libraries to retrieve the data. We clean our data and apply some preprocessing on it. We mention the possible hyper-parameters that can be used in our case, and tweak them in order to find the best hyper-parameters for this task. Finally, we create a multi-input model where inputs are different size sequences of price data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "N713PRCmvpKS",
        "outputId": "f94ada8d-e85f-477d-ab34-0fdc295ed726"
      },
      "source": [
        "# Necessary dependencies\n",
        "!pip install yfinance\n",
        "!pip install finvizfinance"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.1.54)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from yfinance) (0.0.9)\n",
            "Requirement already satisfied: requests>=2.20 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from yfinance) (1.19.4)\n",
            "Requirement already satisfied: pandas>=0.24 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from yfinance) (0.25.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=0.24->yfinance) (2019.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.6.1->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.20->yfinance) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.20->yfinance) (2019.11.28)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.20->yfinance) (1.25.8)\n",
            "WARNING: You are using pip version 20.3.3; however, version 21.1.1 is available.\n",
            "You should consider upgrading via the 'c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n",
            "Requirement already satisfied: finvizfinance in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.9.2)\n",
            "Requirement already satisfied: requests in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from finvizfinance) (2.23.0)\n",
            "Requirement already satisfied: bs4 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from finvizfinance) (0.0.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from finvizfinance) (0.25.1)\n",
            "Requirement already satisfied: datetime in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from finvizfinance) (4.3)\n",
            "Requirement already satisfied: lxml in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from finvizfinance) (4.4.2)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bs4->finvizfinance) (4.9.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from beautifulsoup4->bs4->finvizfinance) (2.0)\n",
            "Requirement already satisfied: pytz in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from datetime->finvizfinance) (2019.2)\n",
            "Requirement already satisfied: zope.interface in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from datetime->finvizfinance) (5.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas->finvizfinance) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas->finvizfinance) (1.19.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.6.1->pandas->finvizfinance) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->finvizfinance) (1.25.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->finvizfinance) (2019.11.28)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->finvizfinance) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->finvizfinance) (2.9)\n",
            "Requirement already satisfied: setuptools in c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from zope.interface->datetime->finvizfinance) (42.0.2)\n",
            "WARNING: You are using pip version 20.3.3; however, version 21.1.1 is available.\n",
            "You should consider upgrading via the 'c:\\users\\peter\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoH2RLouzfLC"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import *\n",
        "import yfinance as yf\n",
        "from finvizfinance.screener.overview import Overview\n",
        "import matplotlib.pyplot as pl"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlN1WfPKzfLD"
      },
      "source": [
        "### API Docs\n",
        "\n",
        "\"yfinance\" library is used to download historical data\n",
        " - https://pypi.org/project/yfinance/\n",
        "\n",
        "\"finvizfinance\" library is used to get ticker names\n",
        " - https://pypi.org/project/finvizfinance/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b83ibCrzfLE"
      },
      "source": [
        "## Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYt1InKHzfLE"
      },
      "source": [
        "# Get an overview of the screener\n",
        "overview = Overview()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dbi553FCzfLE",
        "outputId": "b5e28379-85c0-415a-837b-6d264cfe4bc8"
      },
      "source": [
        "# Here are the options for filters to use to filter stocks\n",
        "overview.getFilters()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Exchange',\n",
              " 'Index',\n",
              " 'Sector',\n",
              " 'Industry',\n",
              " 'Country',\n",
              " 'Market Cap.',\n",
              " 'P/E',\n",
              " 'Forward P/E',\n",
              " 'PEG',\n",
              " 'P/S',\n",
              " 'P/B',\n",
              " 'Price/Cash',\n",
              " 'Price/Free Cash Flow',\n",
              " 'EPS growththis year',\n",
              " 'EPS growthnext year',\n",
              " 'EPS growthpast 5 years',\n",
              " 'EPS growthnext 5 years',\n",
              " 'Sales growthpast 5 years',\n",
              " 'EPS growthqtr over qtr',\n",
              " 'Sales growthqtr over qtr',\n",
              " 'Dividend Yield',\n",
              " 'Return on Assets',\n",
              " 'Return on Equity',\n",
              " 'Return on Investment',\n",
              " 'Current Ratio',\n",
              " 'Quick Ratio',\n",
              " 'LT Debt/Equity',\n",
              " 'Debt/Equity',\n",
              " 'Gross Margin',\n",
              " 'Operating Margin',\n",
              " 'Net Profit Margin',\n",
              " 'Payout Ratio',\n",
              " 'InsiderOwnership',\n",
              " 'InsiderTransactions',\n",
              " 'InstitutionalOwnership',\n",
              " 'InstitutionalTransactions',\n",
              " 'Float Short',\n",
              " 'Analyst Recom.',\n",
              " 'Option/Short',\n",
              " 'Earnings Date',\n",
              " 'Performance',\n",
              " 'Performance 2',\n",
              " 'Volatility',\n",
              " 'RSI (14)',\n",
              " 'Gap',\n",
              " '20-Day Simple Moving Average',\n",
              " '50-Day Simple Moving Average',\n",
              " '200-Day Simple Moving Average',\n",
              " 'Change',\n",
              " 'Change from Open',\n",
              " '20-Day High/Low',\n",
              " '50-Day High/Low',\n",
              " '52-Week High/Low',\n",
              " 'Pattern',\n",
              " 'Candlestick',\n",
              " 'Beta',\n",
              " 'Average True Range',\n",
              " 'Average Volume',\n",
              " 'Relative Volume',\n",
              " 'Current Volume',\n",
              " 'Price',\n",
              " 'Target Price',\n",
              " 'IPO Date',\n",
              " 'Shares Outstanding',\n",
              " 'Float']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bGJPz2qAzfLG",
        "outputId": "c896dfe9-4fbb-4b98-9096-fc0510c58872"
      },
      "source": [
        "# We would like to retrieve stocks with the highest Market Capitalization.\n",
        "# \"Market capitalization refers to the total dollar market value of a company's outstanding shares of stock.\"\n",
        "# In other words, these are the most valued stocks. The reasoning behind selecting this filter is simply that\n",
        "# these are strong stocks that are traded by most investors or hedge funds.\n",
        "overview.getFilterOptions('Market Cap.')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Any',\n",
              " 'Mega ($200bln and more)',\n",
              " 'Large ($10bln to $200bln)',\n",
              " 'Mid ($2bln to $10bln)',\n",
              " 'Small ($300mln to $2bln)',\n",
              " 'Micro ($50mln to $300mln)',\n",
              " 'Nano (under $50mln)',\n",
              " '+Large (over $10bln)',\n",
              " '+Mid (over $2bln)',\n",
              " '+Small (over $300mln)',\n",
              " '+Micro (over $50mln)',\n",
              " '-Large (under $200bln)',\n",
              " '-Mid (under $10bln)',\n",
              " '-Small (under $2bln)',\n",
              " '-Micro (under $300mln)']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GUdykmplzfLG",
        "outputId": "99be9b4e-c0d6-4927-c6ec-f425738dff90"
      },
      "source": [
        "filters = {'Market Cap.':'+Large (over $10bln)'}\n",
        "overview.set_filter(filters_dict=filters)\n",
        "df = overview.ScreenerView()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "KwmUXfNVzfLH",
        "outputId": "d4ac86bf-c5ea-437f-90d2-294ba9aeaf2a"
      },
      "source": [
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Ticker                                  Company                  Sector  \\\n",
              "0        A               Agilent Technologies, Inc.              Healthcare   \n",
              "1      AAL             American Airlines Group Inc.             Industrials   \n",
              "2      AAP                 Advance Auto Parts, Inc.       Consumer Cyclical   \n",
              "3     AAPL                               Apple Inc.              Technology   \n",
              "4      ABB                                  ABB Ltd             Industrials   \n",
              "..     ...                                      ...                     ...   \n",
              "878   ZNGA                               Zynga Inc.  Communication Services   \n",
              "879    ZNH  China Southern Airlines Company Limited             Industrials   \n",
              "880     ZS                            Zscaler, Inc.              Technology   \n",
              "881    ZTO                ZTO Express (Cayman) Inc.             Industrials   \n",
              "882    ZTS                              Zoetis Inc.              Healthcare   \n",
              "\n",
              "                                     Industry      Country    Market Cap  \\\n",
              "0                      Diagnostics & Research          USA  3.962000e+10   \n",
              "1                                    Airlines          USA  1.360000e+10   \n",
              "2                            Specialty Retail          USA  1.338000e+10   \n",
              "3                        Consumer Electronics          USA  2.085450e+12   \n",
              "4              Specialty Industrial Machinery  Switzerland  7.178000e+10   \n",
              "..                                        ...          ...           ...   \n",
              "878            Electronic Gaming & Multimedia          USA  1.075000e+10   \n",
              "879                                  Airlines        China  1.342000e+10   \n",
              "880                 Software - Infrastructure          USA  2.188000e+10   \n",
              "881            Integrated Freight & Logistics        China  2.566000e+10   \n",
              "882  Drug Manufacturers - Specialty & Generic          USA  8.111000e+10   \n",
              "\n",
              "        P/E   Price  Change      Volume  \n",
              "0     50.35  130.45 -0.0053    150881.0  \n",
              "1      None   22.83  0.0192  12984725.0  \n",
              "2     28.38  203.96  0.0057     79507.0  \n",
              "3      28.6  126.10 -0.0106  22937746.0  \n",
              "4    139.11   33.86 -0.0105    454878.0  \n",
              "..      ...     ...     ...         ...  \n",
              "878    None   10.09 -0.0142   2999781.0  \n",
              "879    None   33.01  0.0003      1077.0  \n",
              "880    None  164.24 -0.0067    316464.0  \n",
              "881   36.79   30.92  0.0029    545857.0  \n",
              "882   46.45  170.85 -0.0084    156803.0  \n",
              "\n",
              "[883 rows x 10 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ticker</th>\n      <th>Company</th>\n      <th>Sector</th>\n      <th>Industry</th>\n      <th>Country</th>\n      <th>Market Cap</th>\n      <th>P/E</th>\n      <th>Price</th>\n      <th>Change</th>\n      <th>Volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>A</td>\n      <td>Agilent Technologies, Inc.</td>\n      <td>Healthcare</td>\n      <td>Diagnostics &amp; Research</td>\n      <td>USA</td>\n      <td>3.962000e+10</td>\n      <td>50.35</td>\n      <td>130.45</td>\n      <td>-0.0053</td>\n      <td>150881.0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>AAL</td>\n      <td>American Airlines Group Inc.</td>\n      <td>Industrials</td>\n      <td>Airlines</td>\n      <td>USA</td>\n      <td>1.360000e+10</td>\n      <td>None</td>\n      <td>22.83</td>\n      <td>0.0192</td>\n      <td>12984725.0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>AAP</td>\n      <td>Advance Auto Parts, Inc.</td>\n      <td>Consumer Cyclical</td>\n      <td>Specialty Retail</td>\n      <td>USA</td>\n      <td>1.338000e+10</td>\n      <td>28.38</td>\n      <td>203.96</td>\n      <td>0.0057</td>\n      <td>79507.0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>AAPL</td>\n      <td>Apple Inc.</td>\n      <td>Technology</td>\n      <td>Consumer Electronics</td>\n      <td>USA</td>\n      <td>2.085450e+12</td>\n      <td>28.6</td>\n      <td>126.10</td>\n      <td>-0.0106</td>\n      <td>22937746.0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>ABB</td>\n      <td>ABB Ltd</td>\n      <td>Industrials</td>\n      <td>Specialty Industrial Machinery</td>\n      <td>Switzerland</td>\n      <td>7.178000e+10</td>\n      <td>139.11</td>\n      <td>33.86</td>\n      <td>-0.0105</td>\n      <td>454878.0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>878</td>\n      <td>ZNGA</td>\n      <td>Zynga Inc.</td>\n      <td>Communication Services</td>\n      <td>Electronic Gaming &amp; Multimedia</td>\n      <td>USA</td>\n      <td>1.075000e+10</td>\n      <td>None</td>\n      <td>10.09</td>\n      <td>-0.0142</td>\n      <td>2999781.0</td>\n    </tr>\n    <tr>\n      <td>879</td>\n      <td>ZNH</td>\n      <td>China Southern Airlines Company Limited</td>\n      <td>Industrials</td>\n      <td>Airlines</td>\n      <td>China</td>\n      <td>1.342000e+10</td>\n      <td>None</td>\n      <td>33.01</td>\n      <td>0.0003</td>\n      <td>1077.0</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>ZS</td>\n      <td>Zscaler, Inc.</td>\n      <td>Technology</td>\n      <td>Software - Infrastructure</td>\n      <td>USA</td>\n      <td>2.188000e+10</td>\n      <td>None</td>\n      <td>164.24</td>\n      <td>-0.0067</td>\n      <td>316464.0</td>\n    </tr>\n    <tr>\n      <td>881</td>\n      <td>ZTO</td>\n      <td>ZTO Express (Cayman) Inc.</td>\n      <td>Industrials</td>\n      <td>Integrated Freight &amp; Logistics</td>\n      <td>China</td>\n      <td>2.566000e+10</td>\n      <td>36.79</td>\n      <td>30.92</td>\n      <td>0.0029</td>\n      <td>545857.0</td>\n    </tr>\n    <tr>\n      <td>882</td>\n      <td>ZTS</td>\n      <td>Zoetis Inc.</td>\n      <td>Healthcare</td>\n      <td>Drug Manufacturers - Specialty &amp; Generic</td>\n      <td>USA</td>\n      <td>8.111000e+10</td>\n      <td>46.45</td>\n      <td>170.85</td>\n      <td>-0.0084</td>\n      <td>156803.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>883 rows × 10 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk5hjEAfzfLH"
      },
      "source": [
        "tickers = df['Ticker'].values.tolist()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NPwmddCYzfLH",
        "outputId": "d386549c-9eb2-46d2-9c4c-ad8c098126c8"
      },
      "source": [
        "# Now that we have the tickers, we can download historical data for these tickers.\n",
        "# We will download daily data from 2019 to 2021 (2 years).\n",
        "data = yf.download(tickers, start='2018-01-02', end='2021-05-03')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%***********************]  883 of 883 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "eqUs2NtYzfLI",
        "outputId": "d8555a3b-e1fb-4ddd-bcb7-2ebda9e72ed7"
      },
      "source": [
        "data"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Adj Close                                                \\\n",
              "                     A        AAL         AAP        AAPL        ABB   \n",
              "Date                                                                   \n",
              "2018-01-02   65.777748  51.647556  104.796822   41.310070  23.338377   \n",
              "2018-01-03   67.451401  51.014027  105.745117   41.302879  23.511126   \n",
              "2018-01-04   66.945419  51.335667  109.646957   41.494736  23.709787   \n",
              "2018-01-05   68.015762  51.316177  110.812576   41.967163  23.865261   \n",
              "2018-01-08   68.161713  50.809345  110.032219   41.811283  23.839348   \n",
              "...                ...        ...         ...         ...        ...   \n",
              "2021-04-27  136.479996  21.760000  197.279999  134.162109  33.060001   \n",
              "2021-04-28  134.800003  21.700001  197.630005  133.353485  33.290001   \n",
              "2021-04-29  134.149994  21.299999  200.679993  133.253662  33.240002   \n",
              "2021-04-30  133.639999  21.719999  200.160004  131.237091  32.430000   \n",
              "2021-05-03         NaN        NaN         NaN         NaN        NaN   \n",
              "\n",
              "                                                                      ...  \\\n",
              "                  ABBV         ABC      ABEV        ABMD        ABNB  ...   \n",
              "Date                                                                  ...   \n",
              "2018-01-02   82.956924   88.406036  6.048749  192.490005         NaN  ...   \n",
              "2018-01-03   84.255081   88.735046  6.085353  195.820007         NaN  ...   \n",
              "2018-01-04   83.774597   88.537621  6.076201  199.250000         NaN  ...   \n",
              "2018-01-05   85.232941   89.609322  6.103654  202.320007         NaN  ...   \n",
              "2018-01-08   83.867310   91.094681  6.057900  207.800003         NaN  ...   \n",
              "...                ...         ...       ...         ...         ...  ...   \n",
              "2021-04-27  111.440002  118.362434  2.820000  340.630005  177.940002  ...   \n",
              "2021-04-28  111.930000  118.890480  2.850000  351.170013  180.000000  ...   \n",
              "2021-04-29  110.889999  120.803406  2.890000  322.940002  177.679993  ...   \n",
              "2021-04-30  111.500000  120.355064  2.780000  320.730011  172.710007  ...   \n",
              "2021-05-03         NaN         NaN       NaN         NaN         NaN  ...   \n",
              "\n",
              "               Volume                                                        \\\n",
              "                  ZEN        ZG         ZI      ZLAB         ZM        ZNGA   \n",
              "Date                                                                          \n",
              "2018-01-02   840300.0  277300.0        NaN   72300.0        NaN  12411100.0   \n",
              "2018-01-03   860000.0  372700.0        NaN   79100.0        NaN  23113000.0   \n",
              "2018-01-04   441200.0  255100.0        NaN   58000.0        NaN  10648300.0   \n",
              "2018-01-05   490300.0  127300.0        NaN   48300.0        NaN   9962100.0   \n",
              "2018-01-08   549900.0  218600.0        NaN   92600.0        NaN   8992800.0   \n",
              "...               ...       ...        ...       ...        ...         ...   \n",
              "2021-04-27   866100.0  317800.0  1608900.0  394200.0  2208300.0  17700700.0   \n",
              "2021-04-28  1164100.0  399000.0  1174500.0  353100.0  1437600.0  12559000.0   \n",
              "2021-04-29  1849400.0  303300.0   918100.0  420700.0  3121100.0   8748300.0   \n",
              "2021-04-30  2576300.0  353800.0  1110200.0  421500.0  2147200.0  11300800.0   \n",
              "2021-05-03        NaN       NaN        NaN       NaN        NaN         NaN   \n",
              "\n",
              "                                                      \n",
              "                ZNH         ZS        ZTO        ZTS  \n",
              "Date                                                  \n",
              "2018-01-02  38200.0        NaN  4857000.0  2135600.0  \n",
              "2018-01-03  36300.0        NaN  2629800.0  2328200.0  \n",
              "2018-01-04  14800.0        NaN  2320200.0  2534000.0  \n",
              "2018-01-05  72700.0        NaN  1770800.0  2166100.0  \n",
              "2018-01-08  55200.0        NaN  1337700.0  3631400.0  \n",
              "...             ...        ...        ...        ...  \n",
              "2021-04-27  10600.0  1365900.0  3011500.0  1323100.0  \n",
              "2021-04-28   4700.0  2200000.0  3514300.0  1394600.0  \n",
              "2021-04-29   9800.0  2168500.0  3063100.0  1686200.0  \n",
              "2021-04-30   9200.0  1173800.0  2652200.0  2151000.0  \n",
              "2021-05-03      NaN        NaN        NaN        NaN  \n",
              "\n",
              "[841 rows x 5298 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"10\" halign=\"left\">Adj Close</th>\n      <th>...</th>\n      <th colspan=\"10\" halign=\"left\">Volume</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>A</th>\n      <th>AAL</th>\n      <th>AAP</th>\n      <th>AAPL</th>\n      <th>ABB</th>\n      <th>ABBV</th>\n      <th>ABC</th>\n      <th>ABEV</th>\n      <th>ABMD</th>\n      <th>ABNB</th>\n      <th>...</th>\n      <th>ZEN</th>\n      <th>ZG</th>\n      <th>ZI</th>\n      <th>ZLAB</th>\n      <th>ZM</th>\n      <th>ZNGA</th>\n      <th>ZNH</th>\n      <th>ZS</th>\n      <th>ZTO</th>\n      <th>ZTS</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2018-01-02</td>\n      <td>65.777748</td>\n      <td>51.647556</td>\n      <td>104.796822</td>\n      <td>41.310070</td>\n      <td>23.338377</td>\n      <td>82.956924</td>\n      <td>88.406036</td>\n      <td>6.048749</td>\n      <td>192.490005</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>840300.0</td>\n      <td>277300.0</td>\n      <td>NaN</td>\n      <td>72300.0</td>\n      <td>NaN</td>\n      <td>12411100.0</td>\n      <td>38200.0</td>\n      <td>NaN</td>\n      <td>4857000.0</td>\n      <td>2135600.0</td>\n    </tr>\n    <tr>\n      <td>2018-01-03</td>\n      <td>67.451401</td>\n      <td>51.014027</td>\n      <td>105.745117</td>\n      <td>41.302879</td>\n      <td>23.511126</td>\n      <td>84.255081</td>\n      <td>88.735046</td>\n      <td>6.085353</td>\n      <td>195.820007</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>860000.0</td>\n      <td>372700.0</td>\n      <td>NaN</td>\n      <td>79100.0</td>\n      <td>NaN</td>\n      <td>23113000.0</td>\n      <td>36300.0</td>\n      <td>NaN</td>\n      <td>2629800.0</td>\n      <td>2328200.0</td>\n    </tr>\n    <tr>\n      <td>2018-01-04</td>\n      <td>66.945419</td>\n      <td>51.335667</td>\n      <td>109.646957</td>\n      <td>41.494736</td>\n      <td>23.709787</td>\n      <td>83.774597</td>\n      <td>88.537621</td>\n      <td>6.076201</td>\n      <td>199.250000</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>441200.0</td>\n      <td>255100.0</td>\n      <td>NaN</td>\n      <td>58000.0</td>\n      <td>NaN</td>\n      <td>10648300.0</td>\n      <td>14800.0</td>\n      <td>NaN</td>\n      <td>2320200.0</td>\n      <td>2534000.0</td>\n    </tr>\n    <tr>\n      <td>2018-01-05</td>\n      <td>68.015762</td>\n      <td>51.316177</td>\n      <td>110.812576</td>\n      <td>41.967163</td>\n      <td>23.865261</td>\n      <td>85.232941</td>\n      <td>89.609322</td>\n      <td>6.103654</td>\n      <td>202.320007</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>490300.0</td>\n      <td>127300.0</td>\n      <td>NaN</td>\n      <td>48300.0</td>\n      <td>NaN</td>\n      <td>9962100.0</td>\n      <td>72700.0</td>\n      <td>NaN</td>\n      <td>1770800.0</td>\n      <td>2166100.0</td>\n    </tr>\n    <tr>\n      <td>2018-01-08</td>\n      <td>68.161713</td>\n      <td>50.809345</td>\n      <td>110.032219</td>\n      <td>41.811283</td>\n      <td>23.839348</td>\n      <td>83.867310</td>\n      <td>91.094681</td>\n      <td>6.057900</td>\n      <td>207.800003</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>549900.0</td>\n      <td>218600.0</td>\n      <td>NaN</td>\n      <td>92600.0</td>\n      <td>NaN</td>\n      <td>8992800.0</td>\n      <td>55200.0</td>\n      <td>NaN</td>\n      <td>1337700.0</td>\n      <td>3631400.0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>2021-04-27</td>\n      <td>136.479996</td>\n      <td>21.760000</td>\n      <td>197.279999</td>\n      <td>134.162109</td>\n      <td>33.060001</td>\n      <td>111.440002</td>\n      <td>118.362434</td>\n      <td>2.820000</td>\n      <td>340.630005</td>\n      <td>177.940002</td>\n      <td>...</td>\n      <td>866100.0</td>\n      <td>317800.0</td>\n      <td>1608900.0</td>\n      <td>394200.0</td>\n      <td>2208300.0</td>\n      <td>17700700.0</td>\n      <td>10600.0</td>\n      <td>1365900.0</td>\n      <td>3011500.0</td>\n      <td>1323100.0</td>\n    </tr>\n    <tr>\n      <td>2021-04-28</td>\n      <td>134.800003</td>\n      <td>21.700001</td>\n      <td>197.630005</td>\n      <td>133.353485</td>\n      <td>33.290001</td>\n      <td>111.930000</td>\n      <td>118.890480</td>\n      <td>2.850000</td>\n      <td>351.170013</td>\n      <td>180.000000</td>\n      <td>...</td>\n      <td>1164100.0</td>\n      <td>399000.0</td>\n      <td>1174500.0</td>\n      <td>353100.0</td>\n      <td>1437600.0</td>\n      <td>12559000.0</td>\n      <td>4700.0</td>\n      <td>2200000.0</td>\n      <td>3514300.0</td>\n      <td>1394600.0</td>\n    </tr>\n    <tr>\n      <td>2021-04-29</td>\n      <td>134.149994</td>\n      <td>21.299999</td>\n      <td>200.679993</td>\n      <td>133.253662</td>\n      <td>33.240002</td>\n      <td>110.889999</td>\n      <td>120.803406</td>\n      <td>2.890000</td>\n      <td>322.940002</td>\n      <td>177.679993</td>\n      <td>...</td>\n      <td>1849400.0</td>\n      <td>303300.0</td>\n      <td>918100.0</td>\n      <td>420700.0</td>\n      <td>3121100.0</td>\n      <td>8748300.0</td>\n      <td>9800.0</td>\n      <td>2168500.0</td>\n      <td>3063100.0</td>\n      <td>1686200.0</td>\n    </tr>\n    <tr>\n      <td>2021-04-30</td>\n      <td>133.639999</td>\n      <td>21.719999</td>\n      <td>200.160004</td>\n      <td>131.237091</td>\n      <td>32.430000</td>\n      <td>111.500000</td>\n      <td>120.355064</td>\n      <td>2.780000</td>\n      <td>320.730011</td>\n      <td>172.710007</td>\n      <td>...</td>\n      <td>2576300.0</td>\n      <td>353800.0</td>\n      <td>1110200.0</td>\n      <td>421500.0</td>\n      <td>2147200.0</td>\n      <td>11300800.0</td>\n      <td>9200.0</td>\n      <td>1173800.0</td>\n      <td>2652200.0</td>\n      <td>2151000.0</td>\n    </tr>\n    <tr>\n      <td>2021-05-03</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>841 rows × 5298 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "slcJeRubzfLI",
        "outputId": "9ffbb3bc-0f1d-4df8-9452-d014a92dcc76"
      },
      "source": [
        "# The data contains multiple columns for each ticker including: open, close, low, and high price data.\n",
        "# We will only need one of them.\n",
        "data = data['Close']\n",
        "data.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(841, 883)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh7NMjh8zfLJ"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "j0VEBH3jzfLJ",
        "outputId": "7b6dbcbd-fd5f-4b9e-905d-cbb82b9cbe1c"
      },
      "source": [
        "# First, we see that data for some tickers at some time is unavaiable/missing ('NaN').\n",
        "# To keep the price data in its original form it is best to not replace the unavailable data with zeros or averages.\n",
        "# Additionally, to preserve the size of the data for each ticker, we decided to remove tickers that contain unavailable data.\n",
        "data.dropna(axis=1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: [2018-01-02 00:00:00, 2018-01-03 00:00:00, 2018-01-04 00:00:00, 2018-01-05 00:00:00, 2018-01-08 00:00:00, 2018-01-09 00:00:00, 2018-01-10 00:00:00, 2018-01-11 00:00:00, 2018-01-12 00:00:00, 2018-01-16 00:00:00, 2018-01-17 00:00:00, 2018-01-18 00:00:00, 2018-01-19 00:00:00, 2018-01-22 00:00:00, 2018-01-23 00:00:00, 2018-01-24 00:00:00, 2018-01-25 00:00:00, 2018-01-26 00:00:00, 2018-01-29 00:00:00, 2018-01-30 00:00:00, 2018-01-31 00:00:00, 2018-02-01 00:00:00, 2018-02-02 00:00:00, 2018-02-05 00:00:00, 2018-02-06 00:00:00, 2018-02-07 00:00:00, 2018-02-08 00:00:00, 2018-02-09 00:00:00, 2018-02-12 00:00:00, 2018-02-13 00:00:00, 2018-02-14 00:00:00, 2018-02-15 00:00:00, 2018-02-16 00:00:00, 2018-02-20 00:00:00, 2018-02-21 00:00:00, 2018-02-22 00:00:00, 2018-02-23 00:00:00, 2018-02-26 00:00:00, 2018-02-27 00:00:00, 2018-02-28 00:00:00, 2018-03-01 00:00:00, 2018-03-02 00:00:00, 2018-03-05 00:00:00, 2018-03-06 00:00:00, 2018-03-07 00:00:00, 2018-03-08 00:00:00, 2018-03-09 00:00:00, 2018-03-12 00:00:00, 2018-03-13 00:00:00, 2018-03-14 00:00:00, 2018-03-15 00:00:00, 2018-03-16 00:00:00, 2018-03-19 00:00:00, 2018-03-20 00:00:00, 2018-03-21 00:00:00, 2018-03-22 00:00:00, 2018-03-23 00:00:00, 2018-03-26 00:00:00, 2018-03-27 00:00:00, 2018-03-28 00:00:00, 2018-03-29 00:00:00, 2018-04-02 00:00:00, 2018-04-03 00:00:00, 2018-04-04 00:00:00, 2018-04-05 00:00:00, 2018-04-06 00:00:00, 2018-04-09 00:00:00, 2018-04-10 00:00:00, 2018-04-11 00:00:00, 2018-04-12 00:00:00, 2018-04-13 00:00:00, 2018-04-16 00:00:00, 2018-04-17 00:00:00, 2018-04-18 00:00:00, 2018-04-19 00:00:00, 2018-04-20 00:00:00, 2018-04-23 00:00:00, 2018-04-24 00:00:00, 2018-04-25 00:00:00, 2018-04-26 00:00:00, 2018-04-27 00:00:00, 2018-04-30 00:00:00, 2018-05-01 00:00:00, 2018-05-02 00:00:00, 2018-05-03 00:00:00, 2018-05-04 00:00:00, 2018-05-07 00:00:00, 2018-05-08 00:00:00, 2018-05-09 00:00:00, 2018-05-10 00:00:00, 2018-05-11 00:00:00, 2018-05-14 00:00:00, 2018-05-15 00:00:00, 2018-05-16 00:00:00, 2018-05-17 00:00:00, 2018-05-18 00:00:00, 2018-05-21 00:00:00, 2018-05-22 00:00:00, 2018-05-23 00:00:00, 2018-05-24 00:00:00, ...]\n",
              "\n",
              "[841 rows x 0 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n    </tr>\n    <tr>\n      <th>Date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2018-01-02</td>\n    </tr>\n    <tr>\n      <td>2018-01-03</td>\n    </tr>\n    <tr>\n      <td>2018-01-04</td>\n    </tr>\n    <tr>\n      <td>2018-01-05</td>\n    </tr>\n    <tr>\n      <td>2018-01-08</td>\n    </tr>\n    <tr>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>2021-04-27</td>\n    </tr>\n    <tr>\n      <td>2021-04-28</td>\n    </tr>\n    <tr>\n      <td>2021-04-29</td>\n    </tr>\n    <tr>\n      <td>2021-04-30</td>\n    </tr>\n    <tr>\n      <td>2021-05-03</td>\n    </tr>\n  </tbody>\n</table>\n<p>841 rows × 0 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hrgo-B3XzfLJ",
        "outputId": "caed54ac-955b-4941-fd68-6923d2af72d8"
      },
      "source": [
        "# We find that all tickers contain at least one 'NaN' value. Thus, we experiment by setting a treshold.\n",
        "# And we find that most tickers contain a single missing value. Thus, by setting the treshold to the row length - 1,\n",
        "# we obtain an almost perfect data containing only one missing value.\n",
        "data = data.dropna(axis=1, thresh=len(data)-10)\n",
        "data.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(841, 803)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JdBdukFzzfLK",
        "outputId": "5b71f1ea-b0d9-4852-b48c-151b17b1f347"
      },
      "source": [
        "# By removing rows that contain missing values, we discover that each ticker had a missing value row at the same index.\n",
        "# Thus, now our data is 'NaN' value free and we preserved the most of our data without modifying values.\n",
        "data = data.dropna(axis=0)\n",
        "data.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(838, 803)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdFpon25zfLK"
      },
      "source": [
        "# We must scale the data since different tickers have different prices\n",
        "data -= data.min()\n",
        "data /= data.max()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "DQTT7DXEzfLK",
        "outputId": "c605bb1c-4f43-4b7a-a0fc-9b94e5b31e44"
      },
      "source": [
        "data.head(5)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   A       AAL       AAP      AAPL       ABB      ABBV  \\\n",
              "Date                                                                     \n",
              "2018-01-02  0.084897  0.889136  0.247195  0.069857  0.656773  0.588245   \n",
              "2018-01-03  0.107466  0.875986  0.254835  0.069787  0.667566  0.613814   \n",
              "2018-01-04  0.100643  0.882662  0.286271  0.071646  0.679978  0.604350   \n",
              "2018-01-05  0.115077  0.882258  0.295663  0.076223  0.689692  0.633073   \n",
              "2018-01-08  0.117045  0.871738  0.289375  0.074713  0.688073  0.606176   \n",
              "\n",
              "                 ABC      ABEV      ABMD       ABT  ...         Z       ZBH  \\\n",
              "Date                                                ...                       \n",
              "2018-01-02  0.457457  0.856881  0.194149  0.035019  ...  0.099891  0.443254   \n",
              "2018-01-03  0.464335  0.864220  0.204580  0.036826  ...  0.102693  0.452031   \n",
              "2018-01-04  0.460208  0.862385  0.215324  0.035436  ...  0.097890  0.450194   \n",
              "2018-01-05  0.482610  0.867890  0.224941  0.037799  ...  0.100635  0.462850   \n",
              "2018-01-08  0.513657  0.858716  0.242106  0.035436  ...  0.101492  0.465299   \n",
              "\n",
              "                ZBRA       ZEN        ZG      ZLAB      ZNGA       ZNH  \\\n",
              "Date                                                                     \n",
              "2018-01-02  0.000000  0.000000  0.104227  0.041356  0.068771  0.612565   \n",
              "2018-01-03  0.005027  0.004518  0.106612  0.041865  0.080045  0.675781   \n",
              "2018-01-04  0.010127  0.006374  0.102618  0.040450  0.065389  0.684507   \n",
              "2018-01-05  0.014227  0.007342  0.104449  0.044467  0.065389  0.757223   \n",
              "2018-01-08  0.016887  0.009359  0.106002  0.045429  0.067644  0.741516   \n",
              "\n",
              "                 ZTO       ZTS  \n",
              "Date                            \n",
              "2018-01-02  0.074167  0.002524  \n",
              "2018-01-03  0.069583  0.005728  \n",
              "2018-01-04  0.061667  0.009902  \n",
              "2018-01-05  0.058750  0.017959  \n",
              "2018-01-08  0.044167  0.026502  \n",
              "\n",
              "[5 rows x 803 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>AAL</th>\n      <th>AAP</th>\n      <th>AAPL</th>\n      <th>ABB</th>\n      <th>ABBV</th>\n      <th>ABC</th>\n      <th>ABEV</th>\n      <th>ABMD</th>\n      <th>ABT</th>\n      <th>...</th>\n      <th>Z</th>\n      <th>ZBH</th>\n      <th>ZBRA</th>\n      <th>ZEN</th>\n      <th>ZG</th>\n      <th>ZLAB</th>\n      <th>ZNGA</th>\n      <th>ZNH</th>\n      <th>ZTO</th>\n      <th>ZTS</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2018-01-02</td>\n      <td>0.084897</td>\n      <td>0.889136</td>\n      <td>0.247195</td>\n      <td>0.069857</td>\n      <td>0.656773</td>\n      <td>0.588245</td>\n      <td>0.457457</td>\n      <td>0.856881</td>\n      <td>0.194149</td>\n      <td>0.035019</td>\n      <td>...</td>\n      <td>0.099891</td>\n      <td>0.443254</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.104227</td>\n      <td>0.041356</td>\n      <td>0.068771</td>\n      <td>0.612565</td>\n      <td>0.074167</td>\n      <td>0.002524</td>\n    </tr>\n    <tr>\n      <td>2018-01-03</td>\n      <td>0.107466</td>\n      <td>0.875986</td>\n      <td>0.254835</td>\n      <td>0.069787</td>\n      <td>0.667566</td>\n      <td>0.613814</td>\n      <td>0.464335</td>\n      <td>0.864220</td>\n      <td>0.204580</td>\n      <td>0.036826</td>\n      <td>...</td>\n      <td>0.102693</td>\n      <td>0.452031</td>\n      <td>0.005027</td>\n      <td>0.004518</td>\n      <td>0.106612</td>\n      <td>0.041865</td>\n      <td>0.080045</td>\n      <td>0.675781</td>\n      <td>0.069583</td>\n      <td>0.005728</td>\n    </tr>\n    <tr>\n      <td>2018-01-04</td>\n      <td>0.100643</td>\n      <td>0.882662</td>\n      <td>0.286271</td>\n      <td>0.071646</td>\n      <td>0.679978</td>\n      <td>0.604350</td>\n      <td>0.460208</td>\n      <td>0.862385</td>\n      <td>0.215324</td>\n      <td>0.035436</td>\n      <td>...</td>\n      <td>0.097890</td>\n      <td>0.450194</td>\n      <td>0.010127</td>\n      <td>0.006374</td>\n      <td>0.102618</td>\n      <td>0.040450</td>\n      <td>0.065389</td>\n      <td>0.684507</td>\n      <td>0.061667</td>\n      <td>0.009902</td>\n    </tr>\n    <tr>\n      <td>2018-01-05</td>\n      <td>0.115077</td>\n      <td>0.882258</td>\n      <td>0.295663</td>\n      <td>0.076223</td>\n      <td>0.689692</td>\n      <td>0.633073</td>\n      <td>0.482610</td>\n      <td>0.867890</td>\n      <td>0.224941</td>\n      <td>0.037799</td>\n      <td>...</td>\n      <td>0.100635</td>\n      <td>0.462850</td>\n      <td>0.014227</td>\n      <td>0.007342</td>\n      <td>0.104449</td>\n      <td>0.044467</td>\n      <td>0.065389</td>\n      <td>0.757223</td>\n      <td>0.058750</td>\n      <td>0.017959</td>\n    </tr>\n    <tr>\n      <td>2018-01-08</td>\n      <td>0.117045</td>\n      <td>0.871738</td>\n      <td>0.289375</td>\n      <td>0.074713</td>\n      <td>0.688073</td>\n      <td>0.606176</td>\n      <td>0.513657</td>\n      <td>0.858716</td>\n      <td>0.242106</td>\n      <td>0.035436</td>\n      <td>...</td>\n      <td>0.101492</td>\n      <td>0.465299</td>\n      <td>0.016887</td>\n      <td>0.009359</td>\n      <td>0.106002</td>\n      <td>0.045429</td>\n      <td>0.067644</td>\n      <td>0.741516</td>\n      <td>0.044167</td>\n      <td>0.026502</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 803 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z6gnFd4zfLL"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR28xOxmzfLL"
      },
      "source": [
        "The idea behind our learning is to provide sequences of prices to a model and make the model predict\n",
        "the stock's future direction. We define this direction by the average of the 'n' future prices in contrast\n",
        "to the last price in the sequence.\n",
        "\n",
        "We define parameters such as SEQ_LEN for the length of sequences that go into the model, and N_FUTURE for the number of future prices that we aggregate to compute the labels for the sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN6i-rJfzfLL"
      },
      "source": [
        "SEQ_LEN = 10        # Previous 10 days\n",
        "N_FUTURE = 10       # Next 10 days\n",
        "SPLIT = 0.3"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K1n49YdzfLL"
      },
      "source": [
        "def processData(data, SEQ_LEN, N_FUTURE):\n",
        "    train_data, test_data = [], []\n",
        "\n",
        "    for ticker in data.columns:\n",
        "        prices = data[ticker].values\n",
        "        seqs = []\n",
        "        \n",
        "        for i in range(SEQ_LEN, len(prices)-N_FUTURE):\n",
        "            seq = prices[i-SEQ_LEN:i]\n",
        "\n",
        "            # Next, we want to compute the price direction from the current index\n",
        "            future_price_avg = np.mean(prices[i:i+N_FUTURE])\n",
        "            # direction is 0 (negative) if the future price average is below the current price, otherwise direction is 1 (positive)\n",
        "            price_dir = 0 if future_price_avg < prices[i] else 1       \n",
        "            \n",
        "            # Next, we append the direction as the label to the sequences\n",
        "            # We include the price direction with the sequences so that we can shuffle the data while maintaining the labels corresponding\n",
        "            # to the sequences.\n",
        "            seqs.append([seq, price_dir])\n",
        "\n",
        "        split_index = int(len(seqs) * (1 - SPLIT))\n",
        "        train_data.extend(seqs[:split_index])\n",
        "        # We ignore the overlap of sequences between the train data and the test data\n",
        "        test_data.extend(seqs[split_index + SEQ_LEN:])\n",
        "\n",
        "    # Shuffling\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(train_data)\n",
        "    np.random.shuffle(test_data)\n",
        "\n",
        "    # Separating features and labels\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for features, label in train_data:\n",
        "        X_train.append(features)\n",
        "        y_train.append(label)\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "    for features, label in test_data:\n",
        "        X_test.append(features)\n",
        "        y_test.append(label)\n",
        "    X_test = np.array(X_test)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    return (X_train, y_train), (X_test, y_test)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci4jdcnCzfLM"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = processData(data, SEQ_LEN, N_FUTURE)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4VQnKEizfLM"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHe2GpjszfLM"
      },
      "source": [
        "### Linear model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BpWXLU3WzfLM",
        "outputId": "bb9fc44f-c3dd-46c5-96ac-326f100360ce"
      },
      "source": [
        "model1 = models.Sequential([\n",
        "    layers.Input(shape=(SEQ_LEN,)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model1.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_6 (Dense)              (None, 128)               1408      \n_________________________________________________________________\ndense_7 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_8 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndense_9 (Dense)              (None, 64)                4160      \n_________________________________________________________________\ndense_10 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndense_11 (Dense)             (None, 1)                 33        \n=================================================================\nTotal params: 32,449\nTrainable params: 32,449\nNon-trainable params: 0\n_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ndnie2nmzfLN",
        "outputId": "948c0c91-e888-4f0f-eb7f-d2bcf8480f9d"
      },
      "source": [
        "log1 = model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=128)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3589/3589 [==============================] - 33s 9ms/step - loss: 0.6831 - acc: 0.5528 - val_loss: 0.6798 - val_acc: 0.5894\n",
            "Epoch 2/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6804 - acc: 0.5567 - val_loss: 0.6959 - val_acc: 0.5299\n",
            "Epoch 3/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6799 - acc: 0.5566 - val_loss: 0.6886 - val_acc: 0.5272\n",
            "Epoch 4/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6786 - acc: 0.5562 - val_loss: 0.6893 - val_acc: 0.5309\n",
            "Epoch 5/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6775 - acc: 0.5601 - val_loss: 0.6967 - val_acc: 0.5130\n",
            "Epoch 6/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6769 - acc: 0.5601 - val_loss: 0.6914 - val_acc: 0.5574\n",
            "Epoch 7/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6764 - acc: 0.5613 - val_loss: 0.6848 - val_acc: 0.5481\n",
            "Epoch 8/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6758 - acc: 0.5607 - val_loss: 0.6845 - val_acc: 0.5569\n",
            "Epoch 9/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6751 - acc: 0.5625 - val_loss: 0.6879 - val_acc: 0.5470\n",
            "Epoch 10/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6756 - acc: 0.5617 - val_loss: 0.6875 - val_acc: 0.5507\n",
            "Epoch 11/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6749 - acc: 0.5628 - val_loss: 0.6829 - val_acc: 0.5701\n",
            "Epoch 12/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6748 - acc: 0.5627 - val_loss: 0.6934 - val_acc: 0.5395\n",
            "Epoch 13/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6738 - acc: 0.5633 - val_loss: 0.6933 - val_acc: 0.5290\n",
            "Epoch 14/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6743 - acc: 0.5625 - val_loss: 0.6831 - val_acc: 0.5650\n",
            "Epoch 15/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6743 - acc: 0.5628 - val_loss: 0.6913 - val_acc: 0.5213\n",
            "Epoch 16/50\n",
            "3589/3589 [==============================] - 31s 9ms/step - loss: 0.6737 - acc: 0.5634 - val_loss: 0.6914 - val_acc: 0.5485\n",
            "Epoch 17/50\n",
            "3589/3589 [==============================] - 31s 9ms/step - loss: 0.6734 - acc: 0.5638 - val_loss: 0.6876 - val_acc: 0.5444\n",
            "Epoch 18/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6736 - acc: 0.5627 - val_loss: 0.6863 - val_acc: 0.5583\n",
            "Epoch 19/50\n",
            "3589/3589 [==============================] - 32s 9ms/step - loss: 0.6731 - acc: 0.5649 - val_loss: 0.6928 - val_acc: 0.5377\n",
            "Epoch 20/50\n",
            "3589/3589 [==============================] - 31s 9ms/step - loss: 0.6726 - acc: 0.5648 - val_loss: 0.6917 - val_acc: 0.5432\n",
            "Epoch 21/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6724 - acc: 0.5652 - val_loss: 0.6863 - val_acc: 0.5599\n",
            "Epoch 22/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6725 - acc: 0.5644 - val_loss: 0.6878 - val_acc: 0.5418\n",
            "Epoch 23/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6721 - acc: 0.5654 - val_loss: 0.6857 - val_acc: 0.5694\n",
            "Epoch 24/50\n",
            "3589/3589 [==============================] - 29s 8ms/step - loss: 0.6718 - acc: 0.5663 - val_loss: 0.6951 - val_acc: 0.5226\n",
            "Epoch 25/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6717 - acc: 0.5663 - val_loss: 0.6962 - val_acc: 0.5364\n",
            "Epoch 26/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6719 - acc: 0.5649 - val_loss: 0.6917 - val_acc: 0.5455\n",
            "Epoch 27/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6715 - acc: 0.5647 - val_loss: 0.6907 - val_acc: 0.5365\n",
            "Epoch 28/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6712 - acc: 0.5667 - val_loss: 0.6876 - val_acc: 0.5459\n",
            "Epoch 29/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6708 - acc: 0.5654 - val_loss: 0.6858 - val_acc: 0.5547\n",
            "Epoch 30/50\n",
            "3589/3589 [==============================] - 29s 8ms/step - loss: 0.6706 - acc: 0.5652 - val_loss: 0.6931 - val_acc: 0.5187\n",
            "Epoch 31/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6708 - acc: 0.5664 - val_loss: 0.6866 - val_acc: 0.5475\n",
            "Epoch 32/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6708 - acc: 0.5655 - val_loss: 0.6885 - val_acc: 0.5373\n",
            "Epoch 33/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6704 - acc: 0.5665 - val_loss: 0.6880 - val_acc: 0.5558\n",
            "Epoch 34/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6699 - acc: 0.5682 - val_loss: 0.6954 - val_acc: 0.4982\n",
            "Epoch 35/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6700 - acc: 0.5661 - val_loss: 0.6880 - val_acc: 0.5567\n",
            "Epoch 36/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6689 - acc: 0.5684 - val_loss: 0.6926 - val_acc: 0.5310\n",
            "Epoch 37/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6693 - acc: 0.5676 - val_loss: 0.6954 - val_acc: 0.5477\n",
            "Epoch 38/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6696 - acc: 0.5671 - val_loss: 0.6929 - val_acc: 0.5357\n",
            "Epoch 39/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6688 - acc: 0.5695 - val_loss: 0.6953 - val_acc: 0.5219\n",
            "Epoch 40/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6691 - acc: 0.5674 - val_loss: 0.6912 - val_acc: 0.5421\n",
            "Epoch 41/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6676 - acc: 0.5693 - val_loss: 0.6936 - val_acc: 0.5262\n",
            "Epoch 42/50\n",
            "3589/3589 [==============================] - 32s 9ms/step - loss: 0.6683 - acc: 0.5679 - val_loss: 0.6959 - val_acc: 0.5133\n",
            "Epoch 43/50\n",
            "3589/3589 [==============================] - 31s 8ms/step - loss: 0.6681 - acc: 0.5678 - val_loss: 0.7017 - val_acc: 0.5260\n",
            "Epoch 44/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6676 - acc: 0.5693 - val_loss: 0.6953 - val_acc: 0.5169\n",
            "Epoch 45/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6676 - acc: 0.5689 - val_loss: 0.6904 - val_acc: 0.5397\n",
            "Epoch 46/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6669 - acc: 0.5709 - val_loss: 0.6899 - val_acc: 0.5441\n",
            "Epoch 47/50\n",
            "3589/3589 [==============================] - 31s 9ms/step - loss: 0.6665 - acc: 0.5703 - val_loss: 0.6959 - val_acc: 0.5271\n",
            "Epoch 48/50\n",
            "3589/3589 [==============================] - 31s 9ms/step - loss: 0.6670 - acc: 0.5679 - val_loss: 0.6871 - val_acc: 0.5458\n",
            "Epoch 49/50\n",
            "3589/3589 [==============================] - 31s 9ms/step - loss: 0.6659 - acc: 0.5713 - val_loss: 0.6889 - val_acc: 0.5500\n",
            "Epoch 50/50\n",
            "3589/3589 [==============================] - 30s 8ms/step - loss: 0.6665 - acc: 0.5703 - val_loss: 0.6966 - val_acc: 0.5420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "5zIr_tQhzfLN",
        "outputId": "140a0734-228d-4f76-c36c-ca63592332ba"
      },
      "source": [
        "pl.plot(log1.history['acc'])\n",
        "pl.plot(log1.history['val_acc'])\n",
        "pl.legend(['train acc', \"test acc\"])\n",
        "print(f\"Best test acc: {max(log1.history['val_acc'])}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best test acc: 0.5894104838371277\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"252.018125pt\" version=\"1.1\" viewBox=\"0 0 381.965625 252.018125\" width=\"381.965625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 252.018125 \r\nL 381.965625 252.018125 \r\nL 381.965625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 228.14 \r\nL 371.265625 228.14 \r\nL 371.265625 10.7 \r\nL 36.465625 10.7 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m02027d3570\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m02027d3570\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(48.502557 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"113.798835\" xlink:href=\"#m02027d3570\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(107.436335 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"175.913862\" xlink:href=\"#m02027d3570\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(169.551362 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"238.02889\" xlink:href=\"#m02027d3570\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(231.66639 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"300.143918\" xlink:href=\"#m02027d3570\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(293.781418 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"362.258946\" xlink:href=\"#m02027d3570\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(355.896446 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mb769077393\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb769077393\" y=\"214.26703\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.50 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 218.066249)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb769077393\" y=\"170.942492\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.52 -->\r\n      <g transform=\"translate(7.2 174.741711)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb769077393\" y=\"127.617955\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.54 -->\r\n      <g transform=\"translate(7.2 131.417173)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb769077393\" y=\"84.293417\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.56 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 88.092636)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb769077393\" y=\"40.968879\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.58 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 44.768098)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#pe19f8a7ffb)\" d=\"M 51.683807 99.106768 \r\nL 57.89531 92.839033 \r\nL 64.106812 88.985789 \r\nL 70.318315 86.552317 \r\nL 76.529818 83.736659 \r\nL 82.741321 82.958598 \r\nL 88.952824 82.086025 \r\nL 95.164326 81.741798 \r\nL 101.375829 79.440284 \r\nL 107.587332 80.227899 \r\nL 113.798835 80.784394 \r\nL 120.010337 79.077078 \r\nL 126.22184 78.496954 \r\nL 132.433343 77.921608 \r\nL 138.644846 77.68584 \r\nL 144.856349 76.403061 \r\nL 151.067851 76.23792 \r\nL 157.279354 75.705053 \r\nL 163.490857 75.318347 \r\nL 169.70236 74.912661 \r\nL 175.913862 73.667584 \r\nL 182.125365 74.761723 \r\nL 188.336868 73.795023 \r\nL 194.548371 73.436594 \r\nL 200.759874 73.120515 \r\nL 206.971376 73.412965 \r\nL 213.182879 72.483838 \r\nL 219.394382 71.899066 \r\nL 225.605885 72.281124 \r\nL 231.817388 71.130302 \r\nL 238.02889 71.007641 \r\nL 244.240393 70.422869 \r\nL 250.451896 69.154164 \r\nL 256.663399 69.753138 \r\nL 262.874901 68.965524 \r\nL 269.086404 68.701479 \r\nL 275.297907 67.989269 \r\nL 281.50941 67.531806 \r\nL 287.720913 65.532169 \r\nL 293.932415 67.432774 \r\nL 300.143918 65.249144 \r\nL 306.355421 65.876524 \r\nL 312.566924 65.268124 \r\nL 318.778426 63.381593 \r\nL 324.989929 64.390902 \r\nL 331.201432 63.900386 \r\nL 337.412935 63.325039 \r\nL 343.624438 64.004067 \r\nL 349.83594 62.160144 \r\nL 356.047443 62.042131 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#pe19f8a7ffb)\" d=\"M 51.683807 20.583636 \r\nL 57.89531 149.488681 \r\nL 64.106812 155.341304 \r\nL 70.318315 147.362637 \r\nL 76.529818 186.032975 \r\nL 82.741321 89.968629 \r\nL 88.952824 110.075402 \r\nL 95.164326 90.985943 \r\nL 101.375829 112.350059 \r\nL 107.587332 104.474299 \r\nL 113.798835 62.397462 \r\nL 120.010337 128.593259 \r\nL 126.22184 151.546292 \r\nL 132.433343 73.416839 \r\nL 138.644846 168.14379 \r\nL 144.856349 109.149503 \r\nL 151.067851 118.156976 \r\nL 157.279354 88.059631 \r\nL 163.490857 132.582593 \r\nL 169.70236 120.706007 \r\nL 175.913862 84.561847 \r\nL 182.125365 123.632319 \r\nL 188.336868 63.906325 \r\nL 194.548371 165.3546 \r\nL 200.759874 135.417361 \r\nL 206.971376 115.779283 \r\nL 213.182879 135.211677 \r\nL 219.394382 114.853383 \r\nL 225.605885 95.695363 \r\nL 231.817388 173.859162 \r\nL 238.02889 111.344108 \r\nL 244.240393 133.417077 \r\nL 250.451896 93.489267 \r\nL 256.663399 218.256364 \r\nL 262.874901 91.385948 \r\nL 269.086404 147.122608 \r\nL 275.297907 111.024155 \r\nL 281.50941 136.994915 \r\nL 287.720913 166.760686 \r\nL 293.932415 123.003518 \r\nL 300.143918 157.478839 \r\nL 306.355421 185.358596 \r\nL 312.566924 158.050441 \r\nL 318.778426 177.585613 \r\nL 324.989929 128.296032 \r\nL 331.201432 118.682741 \r\nL 337.412935 155.535626 \r\nL 343.624438 114.990635 \r\nL 349.83594 105.960308 \r\nL 356.047443 123.243676 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 228.14 \r\nL 36.465625 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 228.14 \r\nL 371.265625 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 228.14 \r\nL 371.265625 228.14 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 10.7 \r\nL 371.265625 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 288.6875 48.05625 \r\nL 364.265625 48.05625 \r\nQ 366.265625 48.05625 366.265625 46.05625 \r\nL 366.265625 17.7 \r\nQ 366.265625 15.7 364.265625 15.7 \r\nL 288.6875 15.7 \r\nQ 286.6875 15.7 286.6875 17.7 \r\nL 286.6875 46.05625 \r\nQ 286.6875 48.05625 288.6875 48.05625 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_14\">\r\n     <path d=\"M 290.6875 23.798437 \r\nL 310.6875 23.798437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\"/>\r\n    <g id=\"text_12\">\r\n     <!-- train acc -->\r\n     <defs>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n     </defs>\r\n     <g transform=\"translate(318.6875 27.298437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"325.830078\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"380.810547\" xlink:href=\"#DejaVuSans-99\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_16\">\r\n     <path d=\"M 290.6875 38.476562 \r\nL 310.6875 38.476562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\"/>\r\n    <g id=\"text_13\">\r\n     <!-- test acc -->\r\n     <defs>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(318.6875 41.976562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"192.041016\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"223.828125\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"285.107422\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"340.087891\" xlink:href=\"#DejaVuSans-99\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pe19f8a7ffb\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"10.7\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f84Quw7uzfLN"
      },
      "source": [
        "The model's training accuracy is slowly rising, but the test accuracy is not improving over epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUmWgQRQnFli"
      },
      "source": [
        "### LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6P4f-5vszfLO",
        "outputId": "a1bade1d-229d-4d62-b742-1d4f206c6d2e"
      },
      "source": [
        "model2 = models.Sequential([\n",
        "    layers.Input(shape=(SEQ_LEN,)),\n",
        "    layers.Reshape((SEQ_LEN, 1)),\n",
        "    layers.LSTM(64, return_sequences=True),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model2.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nreshape (Reshape)            (None, 10, 1)             0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 10, 64)            16896     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 64)                33024     \n_________________________________________________________________\ndropout (Dropout)            (None, 64)                0         \n_________________________________________________________________\ndense_12 (Dense)             (None, 10)                650       \n_________________________________________________________________\ndense_13 (Dense)             (None, 1)                 11        \n=================================================================\nTotal params: 50,581\nTrainable params: 50,581\nNon-trainable params: 0\n_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pytqMBHDzfLO",
        "outputId": "745aaf04-9f2c-49fb-afa1-0751234c499b"
      },
      "source": [
        "log2 = model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=128)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3589/3589 [==============================] - 62s 16ms/step - loss: 0.6854 - acc: 0.5498 - val_loss: 0.6861 - val_acc: 0.5300\n",
            "Epoch 2/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6839 - acc: 0.5538 - val_loss: 0.7015 - val_acc: 0.4932\n",
            "Epoch 3/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6833 - acc: 0.5522 - val_loss: 0.6842 - val_acc: 0.5574\n",
            "Epoch 4/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6824 - acc: 0.5519 - val_loss: 0.6845 - val_acc: 0.5411\n",
            "Epoch 5/50\n",
            "3589/3589 [==============================] - 58s 16ms/step - loss: 0.6812 - acc: 0.5525 - val_loss: 0.6809 - val_acc: 0.5740\n",
            "Epoch 6/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6802 - acc: 0.5547 - val_loss: 0.6846 - val_acc: 0.5319\n",
            "Epoch 7/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6794 - acc: 0.5552 - val_loss: 0.6838 - val_acc: 0.5483\n",
            "Epoch 8/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6789 - acc: 0.5585 - val_loss: 0.6845 - val_acc: 0.5569\n",
            "Epoch 9/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6771 - acc: 0.5618 - val_loss: 0.6840 - val_acc: 0.5554\n",
            "Epoch 10/50\n",
            "3589/3589 [==============================] - 58s 16ms/step - loss: 0.6772 - acc: 0.5597 - val_loss: 0.6901 - val_acc: 0.5376\n",
            "Epoch 11/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6764 - acc: 0.5611 - val_loss: 0.6852 - val_acc: 0.5507\n",
            "Epoch 12/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6757 - acc: 0.5627 - val_loss: 0.6916 - val_acc: 0.5221\n",
            "Epoch 13/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6751 - acc: 0.5631 - val_loss: 0.6967 - val_acc: 0.5158\n",
            "Epoch 14/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6742 - acc: 0.5629 - val_loss: 0.6857 - val_acc: 0.5577\n",
            "Epoch 15/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6730 - acc: 0.5655 - val_loss: 0.6926 - val_acc: 0.5299\n",
            "Epoch 16/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6719 - acc: 0.5658 - val_loss: 0.6866 - val_acc: 0.5510\n",
            "Epoch 17/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6711 - acc: 0.5659 - val_loss: 0.6938 - val_acc: 0.5016\n",
            "Epoch 18/50\n",
            "3589/3589 [==============================] - 56s 16ms/step - loss: 0.6698 - acc: 0.5665 - val_loss: 0.6914 - val_acc: 0.5270\n",
            "Epoch 19/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6686 - acc: 0.5703 - val_loss: 0.6869 - val_acc: 0.5481\n",
            "Epoch 20/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6688 - acc: 0.5689 - val_loss: 0.6917 - val_acc: 0.5455\n",
            "Epoch 21/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6674 - acc: 0.5695 - val_loss: 0.6862 - val_acc: 0.5527\n",
            "Epoch 22/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6663 - acc: 0.5722 - val_loss: 0.7002 - val_acc: 0.5097\n",
            "Epoch 23/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6657 - acc: 0.5719 - val_loss: 0.6935 - val_acc: 0.5157\n",
            "Epoch 24/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6643 - acc: 0.5750 - val_loss: 0.6895 - val_acc: 0.5417\n",
            "Epoch 25/50\n",
            "3589/3589 [==============================] - 56s 16ms/step - loss: 0.6642 - acc: 0.5750 - val_loss: 0.6947 - val_acc: 0.5279\n",
            "Epoch 26/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6631 - acc: 0.5759 - val_loss: 0.6908 - val_acc: 0.5463\n",
            "Epoch 27/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6630 - acc: 0.5768 - val_loss: 0.6922 - val_acc: 0.5294\n",
            "Epoch 28/50\n",
            "3589/3589 [==============================] - 56s 16ms/step - loss: 0.6623 - acc: 0.5787 - val_loss: 0.6933 - val_acc: 0.5357\n",
            "Epoch 29/50\n",
            "3589/3589 [==============================] - 56s 16ms/step - loss: 0.6617 - acc: 0.5780 - val_loss: 0.6934 - val_acc: 0.5491\n",
            "Epoch 30/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6603 - acc: 0.5804 - val_loss: 0.6933 - val_acc: 0.5365\n",
            "Epoch 31/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6600 - acc: 0.5823 - val_loss: 0.6915 - val_acc: 0.5326\n",
            "Epoch 32/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6591 - acc: 0.5807 - val_loss: 0.6959 - val_acc: 0.5292\n",
            "Epoch 33/50\n",
            "3589/3589 [==============================] - 56s 16ms/step - loss: 0.6589 - acc: 0.5822 - val_loss: 0.7026 - val_acc: 0.5034\n",
            "Epoch 34/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6581 - acc: 0.5828 - val_loss: 0.6988 - val_acc: 0.5282\n",
            "Epoch 35/50\n",
            "3589/3589 [==============================] - 56s 16ms/step - loss: 0.6577 - acc: 0.5837 - val_loss: 0.6933 - val_acc: 0.5283\n",
            "Epoch 36/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6572 - acc: 0.5840 - val_loss: 0.6947 - val_acc: 0.5360\n",
            "Epoch 37/50\n",
            "3589/3589 [==============================] - 58s 16ms/step - loss: 0.6566 - acc: 0.5851 - val_loss: 0.6890 - val_acc: 0.5502\n",
            "Epoch 38/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6561 - acc: 0.5861 - val_loss: 0.6966 - val_acc: 0.5149\n",
            "Epoch 39/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6555 - acc: 0.5866 - val_loss: 0.6939 - val_acc: 0.5375\n",
            "Epoch 40/50\n",
            "3589/3589 [==============================] - 56s 16ms/step - loss: 0.6553 - acc: 0.5871 - val_loss: 0.6977 - val_acc: 0.5241\n",
            "Epoch 41/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6538 - acc: 0.5897 - val_loss: 0.6989 - val_acc: 0.5250\n",
            "Epoch 42/50\n",
            "3589/3589 [==============================] - 56s 16ms/step - loss: 0.6530 - acc: 0.5900 - val_loss: 0.6995 - val_acc: 0.5290\n",
            "Epoch 43/50\n",
            "3589/3589 [==============================] - 56s 16ms/step - loss: 0.6530 - acc: 0.5901 - val_loss: 0.7024 - val_acc: 0.5173\n",
            "Epoch 44/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6533 - acc: 0.5892 - val_loss: 0.7010 - val_acc: 0.5255\n",
            "Epoch 45/50\n",
            "3589/3589 [==============================] - 56s 16ms/step - loss: 0.6515 - acc: 0.5918 - val_loss: 0.6972 - val_acc: 0.5309\n",
            "Epoch 46/50\n",
            "3589/3589 [==============================] - 56s 16ms/step - loss: 0.6511 - acc: 0.5936 - val_loss: 0.7013 - val_acc: 0.5271\n",
            "Epoch 47/50\n",
            "3589/3589 [==============================] - 56s 16ms/step - loss: 0.6504 - acc: 0.5920 - val_loss: 0.7019 - val_acc: 0.5251\n",
            "Epoch 48/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6498 - acc: 0.5939 - val_loss: 0.7043 - val_acc: 0.5403\n",
            "Epoch 49/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6499 - acc: 0.5934 - val_loss: 0.6988 - val_acc: 0.5354\n",
            "Epoch 50/50\n",
            "3589/3589 [==============================] - 57s 16ms/step - loss: 0.6496 - acc: 0.5924 - val_loss: 0.7009 - val_acc: 0.5354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "oSJQJTcKzfLO",
        "outputId": "42feb99e-720c-4090-df51-ff90b919c352"
      },
      "source": [
        "pl.plot(log2.history['acc'])\n",
        "pl.plot(log2.history['val_acc'])\n",
        "pl.legend(['train acc', \"test acc\"])\n",
        "print(f\"Best test acc: {max(log2.history['val_acc'])}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best test acc: 0.5740285515785217\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"252.018125pt\" version=\"1.1\" viewBox=\"0 0 381.965625 252.018125\" width=\"381.965625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 252.018125 \r\nL 381.965625 252.018125 \r\nL 381.965625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 228.14 \r\nL 371.265625 228.14 \r\nL 371.265625 10.7 \r\nL 36.465625 10.7 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m8767551412\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m8767551412\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(48.502557 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"113.798835\" xlink:href=\"#m8767551412\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(107.436335 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"175.913862\" xlink:href=\"#m8767551412\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(169.551362 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"238.02889\" xlink:href=\"#m8767551412\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(231.66639 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"300.143918\" xlink:href=\"#m8767551412\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(293.781418 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"362.258946\" xlink:href=\"#m8767551412\" y=\"228.14\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(355.896446 242.738437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"me0f676905e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#me0f676905e\" y=\"204.745794\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.50 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 208.545012)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#me0f676905e\" y=\"165.23409\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.52 -->\r\n      <g transform=\"translate(7.2 169.033309)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#me0f676905e\" y=\"125.722386\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.54 -->\r\n      <g transform=\"translate(7.2 129.521605)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#me0f676905e\" y=\"86.210683\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.56 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 90.009902)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#me0f676905e\" y=\"46.698979\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.58 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 50.498198)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#pc05581fc73)\" d=\"M 51.683807 100.950542 \r\nL 57.89531 99.002065 \r\nL 64.106812 99.952694 \r\nL 70.318315 100.369897 \r\nL 76.529818 97.776246 \r\nL 82.741321 95.836483 \r\nL 88.952824 94.331115 \r\nL 95.164326 89.995529 \r\nL 101.375829 88.610506 \r\nL 107.587332 86.907311 \r\nL 113.798835 83.956748 \r\nL 120.010337 81.758869 \r\nL 126.22184 79.720075 \r\nL 132.433343 78.614718 \r\nL 138.644846 75.947942 \r\nL 144.856349 74.967286 \r\nL 151.067851 73.801756 \r\nL 157.279354 70.962941 \r\nL 163.490857 67.866128 \r\nL 169.70236 66.610163 \r\nL 175.913862 65.780115 \r\nL 182.125365 62.511144 \r\nL 188.336868 60.420892 \r\nL 194.548371 58.136935 \r\nL 200.759874 55.491708 \r\nL 206.971376 54.962639 \r\nL 213.182879 52.640058 \r\nL 219.394382 51.082996 \r\nL 225.605885 49.126041 \r\nL 231.817388 46.940997 \r\nL 238.02889 44.596867 \r\nL 244.240393 44.588271 \r\nL 250.451896 43.444173 \r\nL 256.663399 40.13234 \r\nL 262.874901 40.704389 \r\nL 269.086404 38.691501 \r\nL 275.297907 37.117247 \r\nL 281.50941 36.338775 \r\nL 287.720913 35.160175 \r\nL 293.932415 31.039725 \r\nL 300.143918 31.340822 \r\nL 306.355421 29.689204 \r\nL 312.566924 28.493412 \r\nL 318.778426 28.291228 \r\nL 324.989929 26.093348 \r\nL 331.201432 23.555749 \r\nL 337.412935 24.867529 \r\nL 343.624438 22.407293 \r\nL 349.83594 22.235255 \r\nL 356.047443 20.583636 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#pc05581fc73)\" d=\"M 51.683807 145.428603 \r\nL 57.89531 218.256364 \r\nL 64.106812 91.344637 \r\nL 70.318315 123.630326 \r\nL 76.529818 58.496084 \r\nL 82.741321 141.717348 \r\nL 88.952824 109.317085 \r\nL 95.164326 92.397594 \r\nL 101.375829 95.326961 \r\nL 107.587332 130.542017 \r\nL 113.798835 104.594676 \r\nL 120.010337 160.992862 \r\nL 126.22184 173.627754 \r\nL 132.433343 90.698403 \r\nL 138.644846 145.77268 \r\nL 144.856349 104.010852 \r\nL 151.067851 201.566199 \r\nL 157.279354 151.443715 \r\nL 163.490857 109.692367 \r\nL 169.70236 114.8109 \r\nL 175.913862 100.643675 \r\nL 182.125365 185.553767 \r\nL 188.336868 173.638117 \r\nL 194.548371 122.358464 \r\nL 200.759874 149.608989 \r\nL 206.971376 113.288929 \r\nL 213.182879 146.575292 \r\nL 219.394382 134.305319 \r\nL 225.605885 107.669941 \r\nL 231.817388 132.626971 \r\nL 238.02889 140.310069 \r\nL 244.240393 147.002739 \r\nL 250.451896 198.011322 \r\nL 256.663399 148.941795 \r\nL 262.874901 148.764575 \r\nL 269.086404 133.59644 \r\nL 275.297907 105.668358 \r\nL 281.50941 175.28526 \r\nL 287.720913 130.56286 \r\nL 293.932415 157.114868 \r\nL 300.143918 155.446882 \r\nL 306.355421 147.451028 \r\nL 312.566924 170.656702 \r\nL 318.778426 154.393925 \r\nL 324.989929 143.666766 \r\nL 331.201432 151.151803 \r\nL 337.412935 155.15497 \r\nL 343.624438 125.162777 \r\nL 349.83594 134.878664 \r\nL 356.047443 134.764089 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 228.14 \r\nL 36.465625 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 228.14 \r\nL 371.265625 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 228.14 \r\nL 371.265625 228.14 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 10.7 \r\nL 371.265625 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 43.465625 48.05625 \r\nL 119.04375 48.05625 \r\nQ 121.04375 48.05625 121.04375 46.05625 \r\nL 121.04375 17.7 \r\nQ 121.04375 15.7 119.04375 15.7 \r\nL 43.465625 15.7 \r\nQ 41.465625 15.7 41.465625 17.7 \r\nL 41.465625 46.05625 \r\nQ 41.465625 48.05625 43.465625 48.05625 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_14\">\r\n     <path d=\"M 45.465625 23.798437 \r\nL 65.465625 23.798437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\"/>\r\n    <g id=\"text_12\">\r\n     <!-- train acc -->\r\n     <defs>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n     </defs>\r\n     <g transform=\"translate(73.465625 27.298437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"325.830078\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"380.810547\" xlink:href=\"#DejaVuSans-99\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_16\">\r\n     <path d=\"M 45.465625 38.476562 \r\nL 65.465625 38.476562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\"/>\r\n    <g id=\"text_13\">\r\n     <!-- test acc -->\r\n     <defs>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(73.465625 41.976562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"192.041016\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"223.828125\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"285.107422\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"340.087891\" xlink:href=\"#DejaVuSans-99\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pc05581fc73\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"10.7\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyYUo6FWzfLP"
      },
      "source": [
        "The same phenomenon happens with the LSTM model as with the linear model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeQ8DIHHzfLP"
      },
      "source": [
        "## Different parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bPyct9BczfLP",
        "outputId": "8eaefbe1-e48d-4bdf-ac33-1cdfe33d92e2"
      },
      "source": [
        "SEQ_LEN = 30\n",
        "N_FUTURE = 10\n",
        "(X_train, y_train), (X_test, y_test) = processData(data, SEQ_LEN, N_FUTURE)\n",
        "\n",
        "model2 = models.Sequential([\n",
        "    layers.Input(shape=(SEQ_LEN,)),\n",
        "    layers.Reshape((SEQ_LEN, 1)),\n",
        "    layers.LSTM(64, return_sequences=True),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "log2_1 = model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=128)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3501/3501 [==============================] - 61s 17ms/step - loss: 0.6852 - acc: 0.5515 - val_loss: 0.6864 - val_acc: 0.5517\n",
            "Epoch 2/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.6824 - acc: 0.5562 - val_loss: 0.6961 - val_acc: 0.4986\n",
            "Epoch 3/50\n",
            "3501/3501 [==============================] - 59s 17ms/step - loss: 0.6788 - acc: 0.5582 - val_loss: 0.6988 - val_acc: 0.5042\n",
            "Epoch 4/50\n",
            "3501/3501 [==============================] - 58s 16ms/step - loss: 0.6766 - acc: 0.5616 - val_loss: 0.6923 - val_acc: 0.5384\n",
            "Epoch 5/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.6730 - acc: 0.5692 - val_loss: 0.6972 - val_acc: 0.5280\n",
            "Epoch 6/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.6698 - acc: 0.5741 - val_loss: 0.6959 - val_acc: 0.5182\n",
            "Epoch 7/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.6681 - acc: 0.5769 - val_loss: 0.6950 - val_acc: 0.5235\n",
            "Epoch 8/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.6654 - acc: 0.5798 - val_loss: 0.6974 - val_acc: 0.5296\n",
            "Epoch 9/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.6623 - acc: 0.5831 - val_loss: 0.7009 - val_acc: 0.5343\n",
            "Epoch 10/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.6602 - acc: 0.5863 - val_loss: 0.6975 - val_acc: 0.5308\n",
            "Epoch 11/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.6562 - acc: 0.5899 - val_loss: 0.6982 - val_acc: 0.5343\n",
            "Epoch 12/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.6526 - acc: 0.5949 - val_loss: 0.7065 - val_acc: 0.5264\n",
            "Epoch 13/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.6480 - acc: 0.6010 - val_loss: 0.7088 - val_acc: 0.5310\n",
            "Epoch 14/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.6442 - acc: 0.6035 - val_loss: 0.7142 - val_acc: 0.5032\n",
            "Epoch 15/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.6408 - acc: 0.6100 - val_loss: 0.7094 - val_acc: 0.5307\n",
            "Epoch 16/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.6355 - acc: 0.6167 - val_loss: 0.7165 - val_acc: 0.5328\n",
            "Epoch 17/50\n",
            "3501/3501 [==============================] - 58s 16ms/step - loss: 0.6322 - acc: 0.6183 - val_loss: 0.7032 - val_acc: 0.5400\n",
            "Epoch 18/50\n",
            "3501/3501 [==============================] - 58s 16ms/step - loss: 0.6287 - acc: 0.6245 - val_loss: 0.7157 - val_acc: 0.5314\n",
            "Epoch 19/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.6247 - acc: 0.6282 - val_loss: 0.7209 - val_acc: 0.5204\n",
            "Epoch 20/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.6202 - acc: 0.6324 - val_loss: 0.7267 - val_acc: 0.5220\n",
            "Epoch 21/50\n",
            "3501/3501 [==============================] - 58s 16ms/step - loss: 0.6155 - acc: 0.6375 - val_loss: 0.7368 - val_acc: 0.5132\n",
            "Epoch 22/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.6106 - acc: 0.6426 - val_loss: 0.7370 - val_acc: 0.5198\n",
            "Epoch 23/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.6079 - acc: 0.6459 - val_loss: 0.7391 - val_acc: 0.5233\n",
            "Epoch 24/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.6036 - acc: 0.6500 - val_loss: 0.7602 - val_acc: 0.5163\n",
            "Epoch 25/50\n",
            "3501/3501 [==============================] - 58s 16ms/step - loss: 0.6012 - acc: 0.6531 - val_loss: 0.7345 - val_acc: 0.5144\n",
            "Epoch 26/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.5969 - acc: 0.6577 - val_loss: 0.7457 - val_acc: 0.5232\n",
            "Epoch 27/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.5921 - acc: 0.6626 - val_loss: 0.7614 - val_acc: 0.5126\n",
            "Epoch 28/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.5896 - acc: 0.6651 - val_loss: 0.7591 - val_acc: 0.5209\n",
            "Epoch 29/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.5862 - acc: 0.6680 - val_loss: 0.7548 - val_acc: 0.5226\n",
            "Epoch 30/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.5824 - acc: 0.6713 - val_loss: 0.7613 - val_acc: 0.5292\n",
            "Epoch 31/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.5803 - acc: 0.6732 - val_loss: 0.7662 - val_acc: 0.5255\n",
            "Epoch 32/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.5768 - acc: 0.6770 - val_loss: 0.7992 - val_acc: 0.5118\n",
            "Epoch 33/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.5728 - acc: 0.6821 - val_loss: 0.7925 - val_acc: 0.5188\n",
            "Epoch 34/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.5708 - acc: 0.6830 - val_loss: 0.7803 - val_acc: 0.5156\n",
            "Epoch 35/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.5671 - acc: 0.6865 - val_loss: 0.7830 - val_acc: 0.5222\n",
            "Epoch 36/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.5637 - acc: 0.6905 - val_loss: 0.7911 - val_acc: 0.5202\n",
            "Epoch 37/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.5606 - acc: 0.6915 - val_loss: 0.8184 - val_acc: 0.5127\n",
            "Epoch 38/50\n",
            "3501/3501 [==============================] - 57s 16ms/step - loss: 0.5580 - acc: 0.6943 - val_loss: 0.8175 - val_acc: 0.5200\n",
            "Epoch 39/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.5546 - acc: 0.6987 - val_loss: 0.8342 - val_acc: 0.5112\n",
            "Epoch 40/50\n",
            "3501/3501 [==============================] - 58s 16ms/step - loss: 0.5523 - acc: 0.6997 - val_loss: 0.8059 - val_acc: 0.5248\n",
            "Epoch 41/50\n",
            "3501/3501 [==============================] - 56s 16ms/step - loss: 0.5498 - acc: 0.7026 - val_loss: 0.8248 - val_acc: 0.5217\n",
            "Epoch 42/50\n",
            "3501/3501 [==============================] - 58s 17ms/step - loss: 0.5465 - acc: 0.7042 - val_loss: 0.8301 - val_acc: 0.5153\n",
            "Epoch 43/50\n",
            "3501/3501 [==============================] - 56s 16ms/step - loss: 0.5431 - acc: 0.7073 - val_loss: 0.8368 - val_acc: 0.5097\n",
            "Epoch 44/50\n",
            "3501/3501 [==============================] - 56s 16ms/step - loss: 0.5413 - acc: 0.7096 - val_loss: 0.8418 - val_acc: 0.5143\n",
            "Epoch 45/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.5386 - acc: 0.7118 - val_loss: 0.8474 - val_acc: 0.5226\n",
            "Epoch 46/50\n",
            "3501/3501 [==============================] - 56s 16ms/step - loss: 0.5379 - acc: 0.7116 - val_loss: 0.8458 - val_acc: 0.5162\n",
            "Epoch 47/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.5341 - acc: 0.7146 - val_loss: 0.8456 - val_acc: 0.5198\n",
            "Epoch 48/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.5328 - acc: 0.7163 - val_loss: 0.8467 - val_acc: 0.5198\n",
            "Epoch 49/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.5291 - acc: 0.7198 - val_loss: 0.8585 - val_acc: 0.5146\n",
            "Epoch 50/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.5262 - acc: 0.7222 - val_loss: 0.8636 - val_acc: 0.5165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "X5QjI0mOzfLQ",
        "outputId": "fb0477e5-2e09-4cde-9f2f-d2de73139334"
      },
      "source": [
        "SEQ_LEN = 10\n",
        "N_FUTURE = 30\n",
        "(X_train, y_train), (X_test, y_test) = processData(data, SEQ_LEN, N_FUTURE)\n",
        "\n",
        "model2 = models.Sequential([\n",
        "    layers.Input(shape=(SEQ_LEN,)),\n",
        "    layers.Reshape((SEQ_LEN, 1)),\n",
        "    layers.LSTM(64, return_sequences=True),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "log2_2 = model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=128)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3501/3501 [==============================] - 58s 16ms/step - loss: 0.6703 - acc: 0.5855 - val_loss: 0.6633 - val_acc: 0.5724\n",
            "Epoch 2/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6670 - acc: 0.5869 - val_loss: 0.6686 - val_acc: 0.5620\n",
            "Epoch 3/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6649 - acc: 0.5874 - val_loss: 0.6616 - val_acc: 0.5868\n",
            "Epoch 4/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6636 - acc: 0.5879 - val_loss: 0.6646 - val_acc: 0.5584\n",
            "Epoch 5/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6632 - acc: 0.5883 - val_loss: 0.6731 - val_acc: 0.5745\n",
            "Epoch 6/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6617 - acc: 0.5891 - val_loss: 0.6745 - val_acc: 0.5234\n",
            "Epoch 7/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6616 - acc: 0.5876 - val_loss: 0.6598 - val_acc: 0.5697\n",
            "Epoch 8/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6614 - acc: 0.5886 - val_loss: 0.6659 - val_acc: 0.5618\n",
            "Epoch 9/50\n",
            "3501/3501 [==============================] - 54s 16ms/step - loss: 0.6605 - acc: 0.5886 - val_loss: 0.6578 - val_acc: 0.5687\n",
            "Epoch 10/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6594 - acc: 0.5894 - val_loss: 0.6658 - val_acc: 0.5559\n",
            "Epoch 11/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6595 - acc: 0.5890 - val_loss: 0.6625 - val_acc: 0.5537\n",
            "Epoch 12/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6579 - acc: 0.5905 - val_loss: 0.6748 - val_acc: 0.5402\n",
            "Epoch 13/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6569 - acc: 0.5919 - val_loss: 0.6725 - val_acc: 0.5542\n",
            "Epoch 14/50\n",
            "3501/3501 [==============================] - 54s 16ms/step - loss: 0.6566 - acc: 0.5910 - val_loss: 0.6837 - val_acc: 0.5081\n",
            "Epoch 15/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6556 - acc: 0.5913 - val_loss: 0.6714 - val_acc: 0.5616\n",
            "Epoch 16/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6542 - acc: 0.5931 - val_loss: 0.6596 - val_acc: 0.5784\n",
            "Epoch 17/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6541 - acc: 0.5935 - val_loss: 0.6781 - val_acc: 0.5187\n",
            "Epoch 18/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6529 - acc: 0.5941 - val_loss: 0.6726 - val_acc: 0.5392\n",
            "Epoch 19/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6526 - acc: 0.5939 - val_loss: 0.6712 - val_acc: 0.5371\n",
            "Epoch 20/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6520 - acc: 0.5947 - val_loss: 0.6777 - val_acc: 0.5228\n",
            "Epoch 21/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6512 - acc: 0.5948 - val_loss: 0.6796 - val_acc: 0.5164\n",
            "Epoch 22/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6498 - acc: 0.5979 - val_loss: 0.6948 - val_acc: 0.4865\n",
            "Epoch 23/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6499 - acc: 0.5962 - val_loss: 0.6703 - val_acc: 0.5454\n",
            "Epoch 24/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6491 - acc: 0.5979 - val_loss: 0.6836 - val_acc: 0.5223\n",
            "Epoch 25/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6489 - acc: 0.5986 - val_loss: 0.6687 - val_acc: 0.5652\n",
            "Epoch 26/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6481 - acc: 0.5988 - val_loss: 0.6798 - val_acc: 0.5186\n",
            "Epoch 27/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6468 - acc: 0.6004 - val_loss: 0.6797 - val_acc: 0.5126\n",
            "Epoch 28/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6465 - acc: 0.5989 - val_loss: 0.6857 - val_acc: 0.5348\n",
            "Epoch 29/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6467 - acc: 0.6006 - val_loss: 0.6710 - val_acc: 0.5539\n",
            "Epoch 30/50\n",
            "3501/3501 [==============================] - 54s 15ms/step - loss: 0.6456 - acc: 0.6013 - val_loss: 0.6969 - val_acc: 0.5042\n",
            "Epoch 31/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6448 - acc: 0.6024 - val_loss: 0.6830 - val_acc: 0.5426\n",
            "Epoch 32/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6443 - acc: 0.6027 - val_loss: 0.6755 - val_acc: 0.5400\n",
            "Epoch 33/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6431 - acc: 0.6033 - val_loss: 0.6804 - val_acc: 0.5239\n",
            "Epoch 34/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6432 - acc: 0.6038 - val_loss: 0.6936 - val_acc: 0.5138\n",
            "Epoch 35/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6427 - acc: 0.6035 - val_loss: 0.6797 - val_acc: 0.5259\n",
            "Epoch 36/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6421 - acc: 0.6058 - val_loss: 0.6805 - val_acc: 0.5331\n",
            "Epoch 37/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6423 - acc: 0.6042 - val_loss: 0.6971 - val_acc: 0.4881\n",
            "Epoch 38/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6405 - acc: 0.6053 - val_loss: 0.6886 - val_acc: 0.5201\n",
            "Epoch 39/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6405 - acc: 0.6072 - val_loss: 0.6888 - val_acc: 0.5282\n",
            "Epoch 40/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6397 - acc: 0.6065 - val_loss: 0.6932 - val_acc: 0.5079\n",
            "Epoch 41/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6392 - acc: 0.6073 - val_loss: 0.6901 - val_acc: 0.5149\n",
            "Epoch 42/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6382 - acc: 0.6088 - val_loss: 0.6886 - val_acc: 0.5167\n",
            "Epoch 43/50\n",
            "3501/3501 [==============================] - 54s 16ms/step - loss: 0.6379 - acc: 0.6083 - val_loss: 0.6992 - val_acc: 0.5077\n",
            "Epoch 44/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6377 - acc: 0.6086 - val_loss: 0.6786 - val_acc: 0.5487\n",
            "Epoch 45/50\n",
            "3501/3501 [==============================] - 54s 16ms/step - loss: 0.6366 - acc: 0.6102 - val_loss: 0.6878 - val_acc: 0.5305\n",
            "Epoch 46/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6364 - acc: 0.6102 - val_loss: 0.6848 - val_acc: 0.5377\n",
            "Epoch 47/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6349 - acc: 0.6113 - val_loss: 0.6940 - val_acc: 0.5255\n",
            "Epoch 48/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6346 - acc: 0.6124 - val_loss: 0.6782 - val_acc: 0.5485\n",
            "Epoch 49/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6350 - acc: 0.6106 - val_loss: 0.6898 - val_acc: 0.5314\n",
            "Epoch 50/50\n",
            "3501/3501 [==============================] - 55s 16ms/step - loss: 0.6337 - acc: 0.6116 - val_loss: 0.6920 - val_acc: 0.5230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "n2OsOQIazfLR",
        "outputId": "45fa4467-89ed-48bd-83c6-d97f50aa0c50"
      },
      "source": [
        "SEQ_LEN = 30\n",
        "N_FUTURE = 30\n",
        "(X_train, y_train), (X_test, y_test) = processData(data, SEQ_LEN, N_FUTURE)\n",
        "\n",
        "model2 = models.Sequential([\n",
        "    layers.Input(shape=(SEQ_LEN,)),\n",
        "    layers.Reshape((SEQ_LEN, 1)),\n",
        "    layers.LSTM(64, return_sequences=True),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "log2_3 = model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=128)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3413/3413 [==============================] - 59s 17ms/step - loss: 0.6686 - acc: 0.5882 - val_loss: 0.6785 - val_acc: 0.5272\n",
            "Epoch 2/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.6642 - acc: 0.5903 - val_loss: 0.6738 - val_acc: 0.5617\n",
            "Epoch 3/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.6626 - acc: 0.5907 - val_loss: 0.6857 - val_acc: 0.5230\n",
            "Epoch 4/50\n",
            "3413/3413 [==============================] - 57s 17ms/step - loss: 0.6614 - acc: 0.5907 - val_loss: 0.6893 - val_acc: 0.5510\n",
            "Epoch 5/50\n",
            "3413/3413 [==============================] - 57s 17ms/step - loss: 0.6600 - acc: 0.5919 - val_loss: 0.6762 - val_acc: 0.5802\n",
            "Epoch 6/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.6579 - acc: 0.5946 - val_loss: 0.6838 - val_acc: 0.5246\n",
            "Epoch 7/50\n",
            "3413/3413 [==============================] - 57s 17ms/step - loss: 0.6550 - acc: 0.5974 - val_loss: 0.6806 - val_acc: 0.5475\n",
            "Epoch 8/50\n",
            "3413/3413 [==============================] - 56s 17ms/step - loss: 0.6515 - acc: 0.6028 - val_loss: 0.6765 - val_acc: 0.5499\n",
            "Epoch 9/50\n",
            "3413/3413 [==============================] - 56s 17ms/step - loss: 0.6486 - acc: 0.6055 - val_loss: 0.6849 - val_acc: 0.5407\n",
            "Epoch 10/50\n",
            "3413/3413 [==============================] - 56s 17ms/step - loss: 0.6451 - acc: 0.6100 - val_loss: 0.6973 - val_acc: 0.5213\n",
            "Epoch 11/50\n",
            "3413/3413 [==============================] - 56s 17ms/step - loss: 0.6403 - acc: 0.6163 - val_loss: 0.6735 - val_acc: 0.5753\n",
            "Epoch 12/50\n",
            "3413/3413 [==============================] - 56s 17ms/step - loss: 0.6348 - acc: 0.6218 - val_loss: 0.6886 - val_acc: 0.5412\n",
            "Epoch 13/50\n",
            "3413/3413 [==============================] - 57s 17ms/step - loss: 0.6304 - acc: 0.6254 - val_loss: 0.6931 - val_acc: 0.5660\n",
            "Epoch 14/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.6241 - acc: 0.6333 - val_loss: 0.7038 - val_acc: 0.5677\n",
            "Epoch 15/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.6179 - acc: 0.6389 - val_loss: 0.6931 - val_acc: 0.5677\n",
            "Epoch 16/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.6131 - acc: 0.6435 - val_loss: 0.6957 - val_acc: 0.5670\n",
            "Epoch 17/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.6072 - acc: 0.6501 - val_loss: 0.7146 - val_acc: 0.5572\n",
            "Epoch 18/50\n",
            "3413/3413 [==============================] - 58s 17ms/step - loss: 0.6008 - acc: 0.6560 - val_loss: 0.7264 - val_acc: 0.5328\n",
            "Epoch 19/50\n",
            "3413/3413 [==============================] - 57s 17ms/step - loss: 0.5944 - acc: 0.6618 - val_loss: 0.7522 - val_acc: 0.5303\n",
            "Epoch 20/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.5888 - acc: 0.6666 - val_loss: 0.7731 - val_acc: 0.5157\n",
            "Epoch 21/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.5828 - acc: 0.6717 - val_loss: 0.7585 - val_acc: 0.5307\n",
            "Epoch 22/50\n",
            "3413/3413 [==============================] - 56s 17ms/step - loss: 0.5764 - acc: 0.6781 - val_loss: 0.7483 - val_acc: 0.5425\n",
            "Epoch 23/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.5717 - acc: 0.6818 - val_loss: 0.7821 - val_acc: 0.5300\n",
            "Epoch 24/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.5649 - acc: 0.6889 - val_loss: 0.7928 - val_acc: 0.5359\n",
            "Epoch 25/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.5603 - acc: 0.6924 - val_loss: 0.7880 - val_acc: 0.5369\n",
            "Epoch 26/50\n",
            "3413/3413 [==============================] - 57s 17ms/step - loss: 0.5554 - acc: 0.6980 - val_loss: 0.7918 - val_acc: 0.5218\n",
            "Epoch 27/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.5494 - acc: 0.7034 - val_loss: 0.8530 - val_acc: 0.5207\n",
            "Epoch 28/50\n",
            "3413/3413 [==============================] - 57s 17ms/step - loss: 0.5428 - acc: 0.7084 - val_loss: 0.8187 - val_acc: 0.5308\n",
            "Epoch 29/50\n",
            "3413/3413 [==============================] - 56s 17ms/step - loss: 0.5372 - acc: 0.7124 - val_loss: 0.8301 - val_acc: 0.5297\n",
            "Epoch 30/50\n",
            "3413/3413 [==============================] - 57s 17ms/step - loss: 0.5334 - acc: 0.7160 - val_loss: 0.8498 - val_acc: 0.5286\n",
            "Epoch 31/50\n",
            "3413/3413 [==============================] - 57s 17ms/step - loss: 0.5283 - acc: 0.7206 - val_loss: 0.8943 - val_acc: 0.5143\n",
            "Epoch 32/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.5238 - acc: 0.7238 - val_loss: 0.8851 - val_acc: 0.5191\n",
            "Epoch 33/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.5170 - acc: 0.7285 - val_loss: 0.8943 - val_acc: 0.5270\n",
            "Epoch 34/50\n",
            "3413/3413 [==============================] - 56s 17ms/step - loss: 0.5106 - acc: 0.7331 - val_loss: 0.8842 - val_acc: 0.5274\n",
            "Epoch 35/50\n",
            "3413/3413 [==============================] - 57s 17ms/step - loss: 0.5066 - acc: 0.7375 - val_loss: 0.9456 - val_acc: 0.5232\n",
            "Epoch 36/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.5022 - acc: 0.7405 - val_loss: 0.9592 - val_acc: 0.5098\n",
            "Epoch 37/50\n",
            "3413/3413 [==============================] - 56s 17ms/step - loss: 0.4964 - acc: 0.7446 - val_loss: 0.9576 - val_acc: 0.5275\n",
            "Epoch 38/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.4919 - acc: 0.7484 - val_loss: 1.0418 - val_acc: 0.4982\n",
            "Epoch 39/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.4884 - acc: 0.7508 - val_loss: 0.9482 - val_acc: 0.5322\n",
            "Epoch 40/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.4823 - acc: 0.7557 - val_loss: 0.9726 - val_acc: 0.5245\n",
            "Epoch 41/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.4777 - acc: 0.7592 - val_loss: 0.9568 - val_acc: 0.5284\n",
            "Epoch 42/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.4730 - acc: 0.7619 - val_loss: 0.9820 - val_acc: 0.5230\n",
            "Epoch 43/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.4685 - acc: 0.7653 - val_loss: 0.9601 - val_acc: 0.5409\n",
            "Epoch 44/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.4661 - acc: 0.7673 - val_loss: 1.0930 - val_acc: 0.4942\n",
            "Epoch 45/50\n",
            "3413/3413 [==============================] - 57s 17ms/step - loss: 0.4602 - acc: 0.7710 - val_loss: 1.0122 - val_acc: 0.5191\n",
            "Epoch 46/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.4584 - acc: 0.7716 - val_loss: 1.0031 - val_acc: 0.5263\n",
            "Epoch 47/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.4537 - acc: 0.7762 - val_loss: 1.0449 - val_acc: 0.5232\n",
            "Epoch 48/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.4493 - acc: 0.7784 - val_loss: 1.0270 - val_acc: 0.5313\n",
            "Epoch 49/50\n",
            "3413/3413 [==============================] - 56s 16ms/step - loss: 0.4451 - acc: 0.7818 - val_loss: 1.0536 - val_acc: 0.5195\n",
            "Epoch 50/50\n",
            "3413/3413 [==============================] - 57s 17ms/step - loss: 0.4421 - acc: 0.7827 - val_loss: 1.0797 - val_acc: 0.5190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "l-B5Ucm083AC",
        "outputId": "808d26ef-0e43-4058-9415-21c3050763e5"
      },
      "source": [
        "SEQ_LEN = 30\n",
        "N_FUTURE = 60\n",
        "(X_train, y_train), (X_test, y_test) = processData(data, SEQ_LEN, N_FUTURE)\n",
        "\n",
        "model2 = models.Sequential([\n",
        "    layers.Input(shape=(SEQ_LEN,)),\n",
        "    layers.Reshape((SEQ_LEN, 1)),\n",
        "    layers.LSTM(64, return_sequences=True),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "log2_4 = model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=128)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3282/3282 [==============================] - 57s 16ms/step - loss: 0.6502 - acc: 0.6172 - val_loss: 0.6866 - val_acc: 0.4937\n",
            "Epoch 2/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6454 - acc: 0.6212 - val_loss: 0.6807 - val_acc: 0.4943\n",
            "Epoch 3/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6447 - acc: 0.6204 - val_loss: 0.6716 - val_acc: 0.5256\n",
            "Epoch 4/50\n",
            "3282/3282 [==============================] - 54s 16ms/step - loss: 0.6434 - acc: 0.6202 - val_loss: 0.6516 - val_acc: 0.5865\n",
            "Epoch 5/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6428 - acc: 0.6209 - val_loss: 0.6566 - val_acc: 0.5654\n",
            "Epoch 6/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6420 - acc: 0.6214 - val_loss: 0.6874 - val_acc: 0.5366\n",
            "Epoch 7/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6408 - acc: 0.6205 - val_loss: 0.6629 - val_acc: 0.5333\n",
            "Epoch 8/50\n",
            "3282/3282 [==============================] - 54s 16ms/step - loss: 0.6397 - acc: 0.6227 - val_loss: 0.6635 - val_acc: 0.5302\n",
            "Epoch 9/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6383 - acc: 0.6232 - val_loss: 0.6815 - val_acc: 0.5184\n",
            "Epoch 10/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6385 - acc: 0.6229 - val_loss: 0.6618 - val_acc: 0.5404\n",
            "Epoch 11/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6374 - acc: 0.6238 - val_loss: 0.6585 - val_acc: 0.5499\n",
            "Epoch 12/50\n",
            "3282/3282 [==============================] - 54s 16ms/step - loss: 0.6362 - acc: 0.6254 - val_loss: 0.6715 - val_acc: 0.5204\n",
            "Epoch 13/50\n",
            "3282/3282 [==============================] - 54s 16ms/step - loss: 0.6335 - acc: 0.6274 - val_loss: 0.6737 - val_acc: 0.5358\n",
            "Epoch 14/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6331 - acc: 0.6278 - val_loss: 0.6555 - val_acc: 0.5697\n",
            "Epoch 15/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6318 - acc: 0.6280 - val_loss: 0.6682 - val_acc: 0.5547\n",
            "Epoch 16/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6298 - acc: 0.6316 - val_loss: 0.6590 - val_acc: 0.5898\n",
            "Epoch 17/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6286 - acc: 0.6329 - val_loss: 0.6567 - val_acc: 0.5838\n",
            "Epoch 18/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6266 - acc: 0.6338 - val_loss: 0.6534 - val_acc: 0.5946\n",
            "Epoch 19/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6250 - acc: 0.6353 - val_loss: 0.6905 - val_acc: 0.5388\n",
            "Epoch 20/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6217 - acc: 0.6384 - val_loss: 0.6585 - val_acc: 0.6036\n",
            "Epoch 21/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6171 - acc: 0.6433 - val_loss: 0.6554 - val_acc: 0.6043\n",
            "Epoch 22/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6121 - acc: 0.6490 - val_loss: 0.6477 - val_acc: 0.6321\n",
            "Epoch 23/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6126 - acc: 0.6474 - val_loss: 0.6794 - val_acc: 0.5708\n",
            "Epoch 24/50\n",
            "3282/3282 [==============================] - 53s 16ms/step - loss: 0.6034 - acc: 0.6575 - val_loss: 0.6591 - val_acc: 0.6221\n",
            "Epoch 25/50\n",
            "3282/3282 [==============================] - 54s 16ms/step - loss: 0.5996 - acc: 0.6607 - val_loss: 0.6595 - val_acc: 0.6141\n",
            "Epoch 26/50\n",
            "3282/3282 [==============================] - 54s 17ms/step - loss: 0.5936 - acc: 0.6673 - val_loss: 0.6500 - val_acc: 0.6364\n",
            "Epoch 27/50\n",
            " 458/3282 [===>..........................] - ETA: 40s - loss: 0.5900 - acc: 0.6680"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-43-ec0dac4cdd19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m ])\n\u001b[0;32m     14\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mlog2_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \"\"\"\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_supports_tf_logs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1020\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1083\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1085\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m       \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m       \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tX5E17BfZ_S4",
        "outputId": "ac6b6797-3efc-4cc5-9a76-c0f89d59532d"
      },
      "source": [
        "SEQ_LEN = 60\n",
        "N_FUTURE = 30\n",
        "(X_train, y_train), (X_test, y_test) = processData(data, SEQ_LEN, N_FUTURE)\n",
        "\n",
        "model2 = models.Sequential([\n",
        "    layers.Input(shape=(SEQ_LEN,)),\n",
        "    layers.Reshape((SEQ_LEN, 1)),\n",
        "    layers.LSTM(64, return_sequences=True),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "log2_5 = model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Bhk_XbePoZ5L",
        "outputId": "f2a99c48-23cc-41a7-b558-a825d01d8869"
      },
      "source": [
        "SEQ_LEN = 60\n",
        "N_FUTURE = 60\n",
        "(X_train, y_train), (X_test, y_test) = processData(data, SEQ_LEN, N_FUTURE)\n",
        "\n",
        "model2 = models.Sequential([\n",
        "    layers.Input(shape=(SEQ_LEN,)),\n",
        "    layers.Reshape((SEQ_LEN, 1)),\n",
        "    layers.LSTM(64, return_sequences=True),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "log2_6 = model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "3dwUN4A6slX0",
        "outputId": "f45b153c-2096-4f30-f7bb-5ab42f24f559"
      },
      "source": [
        "pl.plot(log2.history['acc'])\n",
        "pl.plot(log2_1.history['acc'])\n",
        "pl.plot(log2_2.history['acc'])\n",
        "pl.plot(log2_3.history['acc'])\n",
        "pl.plot(log2_4.history['acc'])\n",
        "pl.plot(log2_5.history['acc'])\n",
        "pl.plot(log2_6.history['acc'])\n",
        "pl.legend([\"10-10\", \"30-10\", \"10-30\", \"30-30\", \"30-60\", \"60-30\", \"60-60\"])\n",
        "pl.title(\"SEQ_LEN - N_FUTURE parameters\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SiOUICQuSJj"
      },
      "source": [
        "In terms of training accuracy, SEQ_LEN = 60 and N_FUTURE = 60 produces the best results over epochs.\n",
        "\n",
        "We observe that as we increase both parameters the model performs better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "m3_SZ_kiuf_T",
        "outputId": "7c884fa0-2eee-436e-c417-6742d3be3b23"
      },
      "source": [
        "max_val_acc = [max(log2.history['val_acc']), max(log2_1.history['val_acc']), max(log2_2.history['val_acc']), \n",
        "               max(log2_3.history['val_acc']), max(log2_4.history['val_acc']), max(log2_5.history['val_acc']), max(log2_6.history['val_acc'])]\n",
        "pl.bar([\"10-10\", \"30-10\", \"10-30\", \"30-30\", \"30-60\", \"60-30\", \"60-60\"], max_val_acc);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yienxcBRufNw"
      },
      "source": [
        "In terms of testing accuracy, SEQ_LEN = 60 and N_FUTURE = 60 produces the best results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTCseviRUBWQ"
      },
      "source": [
        "Furthermore, we observe that the model is overfitting even with Dropout layers. We suppose the reason is due to the nature of the stock market: \"Past data cannot accurately predict future data.\"\n",
        "\n",
        "Nonetheless, we see some promising results with these high test accuracies at certain epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzGkCcyVCAxC"
      },
      "source": [
        "Different sequence lengths can represent different trends and technical patterns in the stock price. Thus, we decide to make a model with multiple inputs corresponding to sequences with different lengths. We fix the N_FUTURE parameter to 60, and vary the SEQ_LEN between 10, 20, and 30."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENyGxZJRnSHY"
      },
      "source": [
        "## Multi Input model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc_PMmgTu20X"
      },
      "source": [
        "SEQS_LEN = [10, 20, 30]        # Previous 10, 20, 30 days\n",
        "N_FUTURE = 60                  # Next 60 days\n",
        "SPLIT = 0.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f0fPbQP3yJy"
      },
      "source": [
        "train_data, test_data = [[] for _ in range(len(SEQS_LEN) + 1)], [[] for _ in range(len(SEQS_LEN) + 1)]\n",
        "\n",
        "for ticker in data.columns:\n",
        "    prices = data[ticker].values\n",
        "    seqs = [[] for _ in range(len(SEQS_LEN) + 1)]   # +1 for labels\n",
        "    \n",
        "    for i in range(max(SEQS_LEN), len(prices)-N_FUTURE):     # Starting from the largest sequence length size\n",
        "\n",
        "        # We want to compute the price direction from the current index.\n",
        "        future_price_avg = np.mean(prices[i:i+N_FUTURE])\n",
        "        # direction is 0 (negative) if the future price average is below the current price, otherwise direction is 1 (positive)\n",
        "        price_dir = 0 if future_price_avg < prices[i] else 1 \n",
        "    \n",
        "        for j in range(len(SEQS_LEN)):          # For each sequence size\n",
        "            seqs[j].append( prices[i - SEQS_LEN[j] : i] )\n",
        "\n",
        "        seqs[-1].append(price_dir)      # append labels\n",
        "\n",
        "    for j in range(len(train_data)):\n",
        "        split_index = int(len(seqs[j]) * (1 - SPLIT))\n",
        "        train_data[j].extend(np.array(seqs[j])[:split_index])\n",
        "        # We ignore the overlap of sequences between the train data and the test data\n",
        "        test_data[j].extend(np.array(seqs[j])[split_index + max(SEQS_LEN):])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB4u4XIN4eoZ"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "indices = np.arange(len(train_data[0]))\n",
        "np.random.shuffle(indices)\n",
        "for j in range(len(train_data)):\n",
        "    train_data[j] = np.array(train_data[j])[indices]\n",
        "\n",
        "indices = np.arange(len(test_data[0]))\n",
        "np.random.shuffle(indices)\n",
        "for j in range(len(test_data)):\n",
        "    test_data[j] = np.array(test_data[j])[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pkfItRhhEJRy",
        "outputId": "3aed2811-ad36-47e9-d2af-6ec688f420e7"
      },
      "source": [
        "# Make sure the data shapes are fine\n",
        "for j in range(len(test_data)):\n",
        "    print(train_data[j].shape, test_data[j].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAhFWYscFk5_"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1wLsnjuELyw"
      },
      "source": [
        "input1 = layers.Input(shape=(10,))\n",
        "input2 = layers.Input(shape=(20,))\n",
        "input3 = layers.Input(shape=(30,))\n",
        "\n",
        "x = layers.Reshape((10, 1))(input1)\n",
        "x = layers.LSTM(16)(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.Dense(10, activation=\"relu\")(x)\n",
        "x = Model(inputs=input1, outputs=x)\n",
        "\n",
        "y = layers.Reshape((20, 1))(input2)\n",
        "y = layers.LSTM(32)(y)\n",
        "y = layers.Dense(32, activation=\"relu\")(y)\n",
        "y = layers.Dense(10, activation=\"relu\")(y)\n",
        "y = Model(inputs=input2, outputs=y)\n",
        "\n",
        "z = layers.Reshape((30, 1))(input3)\n",
        "z = layers.LSTM(64)(z)\n",
        "z = layers.Dense(64, activation=\"relu\")(z)\n",
        "z = layers.Dense(10, activation=\"relu\")(z)\n",
        "z = Model(inputs=input3, outputs=z)\n",
        "\n",
        "combined = layers.concatenate([x.output, y.output, z.output])\n",
        "\n",
        "c = layers.Dense(3, activation=\"relu\")(combined)\n",
        "c = layers.Dense(1, activation=\"sigmoid\")(c)\n",
        "\n",
        "model3 = Model(inputs=[x.input, y.input, z.input], outputs=c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CCTsr5hLIkT0",
        "outputId": "55506bea-5565-4e7c-b490-51a96bc96b10"
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JdDBYoE3GGWZ",
        "outputId": "f4d1b93b-0e7d-46c7-85dc-42918a795cd4"
      },
      "source": [
        "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "log3 = model3.fit(train_data[:-1], train_data[-1], validation_data=(test_data[:-1], test_data[-1]), epochs=50, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "s4uu3JKbIJ9S",
        "outputId": "c7569a83-70f2-40df-d98f-79d888c892f0"
      },
      "source": [
        "pl.plot(log3.history['acc'])\n",
        "pl.plot(log3.history['val_acc'])\n",
        "pl.legend(['train acc', \"test acc\"])\n",
        "print(f\"Best test acc: {max(log3.history['val_acc'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTB80ip0M1iZ"
      },
      "source": [
        "We observe that the multi sequence model does not solve the overfitting problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "Oun9AoTzJic0",
        "outputId": "d92d9caf-e266-4764-810d-dec44bf82ed1"
      },
      "source": [
        "pl.plot(log2_6.history['acc'])\n",
        "pl.plot(log3.history['acc'])\n",
        "pl.legend([\"60-60\", \"[10,20,30]-60\"])\n",
        "pl.title(\"SEQ_LEN - N_FUTURE parameters\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgeXZKYRLYMz"
      },
      "source": [
        "In terms of training accuracy, when comparing the best parameters (SEQ_LEN=60, N_FUTURE=60) found earlier against the multi sequence model, we observe that the simple sequence model with one input performs significantly better (it converges faster over epochs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "x6I8Fpb6LXam",
        "outputId": "7e066d24-a6e9-4ccc-a3f9-6c2af696712e"
      },
      "source": [
        "max_val_acc = [max(log2_6.history['val_acc']), max(log3.history['val_acc'])]\n",
        "pl.bar([\"60-60\", \"[10,20,30]-60\"], max_val_acc);\n",
        "print([max(log2_6.history['val_acc']), max(log3.history['val_acc'])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7rbx3wXL6g7"
      },
      "source": [
        "In terms of test accuracy, when comparing the best parameters (SEQ_LEN=60, N_FUTURE=60) found earlier against the multi sequence model, we observe that the simple model outperforms once again the multi sequence model by a significant margin."
      ]
    }
  ]
}